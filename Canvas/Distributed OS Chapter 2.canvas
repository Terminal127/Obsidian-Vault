{
	"nodes":[
		{
			"id":"af48ed7db80e3401",
			"type":"text",
			"text":"# CLIENT-SERVER-MODEL\n",
			"styleAttributes":{},
			"x":10000,
			"y":-720,
			"width":480,
			"height":80
		},
		{
			"id":"a2ab3c0162ac5bf8",
			"type":"text",
			"text":"Here’s the information organized into separate headings as provided:\n\n---\n\n## 2.2. ASYNCHRONOUS TRANSFER MODE NETWORKS\n\nThe OSI model, developed in the 1970 s and implemented in the 1980 s, is being surpassed by new developments in the 1990 s, especially in the technology-driven lower layers. This section touches on these advances, which will likely underpin future distributed systems.\n\n### 2.2.1. What Is Asynchronous Transfer Mode?\n\nIn the late 1980 s, telephone companies recognized that telecommunications needed to evolve beyond analog voice channels. Traditional circuit switching (for voice) and packet switching (for data) were inadequate for the varying demands of both types of traffic. The solution, Asynchronous Transfer Mode (ATM), emerged as a hybrid approach using fixed-size blocks over virtual circuits. ATM became an international standard and integrates various types of traffic (voice, data, video) efficiently into a single network, facilitating new applications like video-on-demand and teleconferencing.\n\n### 2.2.2. The ATM Physical Layer\n\nATM adaptors can send cells over a continuous stream on wires or fiber. Although ATM is asynchronous in the virtual circuit context, the physical layer operates synchronously. SONET (Synchronous Optical NETwork) or SDH (Synchronous Digital Hierarchy) may be used, transmitting ATM cells within SONET frames. SONET’s standard frame structure and data rates (e.g., OC-3 c at 155.520 Mbps) are designed to match transmission needs for high-speed data transfer.\n\n### 2.2.3. The ATM Layer\n\nATM cells are 53 bytes in size, with a 5-byte header and a 48-byte payload. The cell header’s fields, including VPI (Virtual Path Identifier) and VCI (Virtual Channel Identifier), help route cells through switches. The cell’s header layout differs between user-to-network and switch-to-switch transmissions, affecting hardware design. The ATM layer handles cell transport but does not guarantee end-to-end reliability, which must be managed by higher layers.\n\n### 2.2.4. The ATM Adaptation Layer\n\nThe adaptation layer is crucial for breaking down packets into cells and reassembling them. SEAL (Simple and Efficient Adaptation Layer) is notable for its simplicity, using a bit in the ATM header to denote the end of a packet. The trailer includes fields for packet length and checksum, allowing for efficient assembly and processing at the destination.\n\n### 2.2.5. ATM Switching\n\nATM networks consist of switches and cables, with switches routing cells based on their VPI and VCI fields. Inside a switch, parallel switching fabrics handle cell routing quickly. Various switch designs address issues like head-of-line blocking and buffer management to ensure efficient switching performance. Options include shared memory, space-division switching, and buffering strategies.\n\n### 2.2.6. Some Implications of ATM for Distributed Systems\n\nATM’s high bandwidth (155 Mbps, 622 Mbps, 2.5 Gbps) has significant implications for distributed systems, especially for wide-area networks. High-speed networks reduce the impact of transmission delays but introduce new challenges in flow control and latency management. For instance, high-speed links reduce the effective bandwidth used due to latency and require new protocols for efficient data transfer. Solutions like rate control and buffer management are essential to address these challenges.\n\n--- \n\nThis format helps to clarify the distinct components and implications of Asynchronous Transfer Mode (ATM) networks.",
			"styleAttributes":{},
			"x":3570,
			"y":3050,
			"width":1160,
			"height":980,
			"color":"1"
		},
		{
			"id":"2cea445fb0c06380",
			"type":"text",
			"text":"### 2.3.2. An Example Client and Server\n\nTo illustrate how clients and servers work, let’s look at a simple example in C. We’ll use a file server and a client that interacts with it. Both need to share some common definitions, which are included in a header file `header.h`.\n\n#### The `header.h` File\n\nThis file defines constants, error codes, and the message format used by both the client and server.\n\n```c\n/* Definitions needed by clients and servers. */\n#define MAX_PATH 255    /* Maximum length of a file name */\n#define BUF_SIZE 1024   /* How much data to transfer at once */\n#define FILE_SERVER 243 /* File server's network address */\n\n/* Definitions of the allowed operations. */\n#define CREATE 1  /* Create a new file */\n#define READ 2    /* Read a piece of a file */\n#define WRITE 3   /* Write a piece of a file */\n#define DELETE 4  /* Delete an existing file */\n\n/* Error codes. */\n#define OK 0         /* Operation performed correctly */\n#define E_BAD_OPCODE 1 /* Unknown operation requested */\n#define E_BAD_PARAM -2 /* Error in a parameter */\n#define E_IO -3       /* Disk error or other I/O error */\n\n/* Definition of the message format. */\nstruct message {\n    long source;    /* Sender’s identity */\n    long dest;      /* Receiver’s identity */\n    long opcode;    /* Operation: CREATE, READ, etc. */\n    long count;     /* Number of bytes to transfer */\n    long offset;    /* Position in file to start reading or writing */\n    long extra1;    /* Extra field for future use */\n    long extra2;    /* Extra field for future use */\n    long result;    /* Result of the operation */\n    char name[MAX_PATH]; /* File name */\n    char data[BUF_SIZE]; /* Data for READ or WRITE */\n};\n```\n\n#### Server Code\n\nHere’s a basic server implementation that handles different file operations based on the received request.\n\n```c\n#include <header.h>\n\nvoid main(void) {\n    struct message m1, m2; /* Incoming and outgoing messages */\n    int r; /* Result code */\n\n    while (1) { /* Server runs forever */\n        receive(FILE_SERVER, &m1); /* Block waiting for a message */\n\n        switch (m1.opcode) { /* Dispatch on type of request */\n            case CREATE: r = do_create(&m1, &m2); break;\n            case READ: r = do_read(&m1, &m2); break;\n            case WRITE: r = do_write(&m1, &m2); break;\n            case DELETE: r = do_delete(&m1, &m2); break;\n            default: r = E_BAD_OPCODE;\n        }\n\n        m2.result = r; /* Return result to client */\n        send(m1.source, &m2); /* Send reply */\n    }\n}\n```\n\n#### Client Code\n\nHere’s a procedure for copying a file using the file server. It reads from the source file and writes to the destination file, communicating with the server for each block of data.\n\n```c\n#include <header.h>\n\nint copy(char *src, char *dst) /* Procedure to copy file using the server */\n{\n    struct message m1; /* Message buffer */\n    long position; /* Current file position */\n    long client = 110; /* Client’s address */\n\n    initialize(); /* Prepare for execution */\n    position = 0;\n\n    do {\n        /* Get a block of data from the source file. */\n        m1.opcode = READ; /* Operation is a read */\n        m1.offset = position; /* Current position in the file */\n        m1.count = BUF_SIZE; /* How many bytes to read */\n        strcpy(m1.name, src); /* Copy name of file to be read to message */\n        send(FILE_SERVER, &m1); /* Send the message to the file server */\n        receive(client, &m1); /* Block waiting for the reply */\n\n        /* Write the data just received to the destination file. */\n        m1.opcode = WRITE; /* Operation is a write */\n        m1.offset = position; /* Current position in the file */\n        m1.count = m1.result; /* Number of bytes to write */\n        strcpy(m1.name, dst); /* Copy name of file to be written to buf */\n        send(FILE_SERVER, &m1); /* Send the message to the file server */\n        receive(client, &m1); /* Block waiting for the reply */\n\n        position += m1.result; /* m1.result is number of bytes written */\n    } while (m1.result > 0); /* Iterate until done */\n\n    return (m1.result >= 0 ? OK : m1.result); /* Return OK or error code */\n}\n```\n\n### Summary\n\nIn this example:\n\n- **Header File (`header.h`)**: Defines the constants and message structure used for communication between the client and server.\n- **Server Code**: Handles different file operations based on the request, sends back results.\n- **Client Code**: Demonstrates how to use the server to copy a file by sending read and write requests.\n\nThis example simplifies the client-server interaction and leaves out many details for clarity, such as actual implementation of `do_create`, `do_read`, `do_write`, and `do_delete` functions, as well as error checking.",
			"styleAttributes":{},
			"x":390,
			"y":4550,
			"width":1140,
			"height":580,
			"color":"1"
		},
		{
			"id":"5a5b0ec268aa5176",
			"type":"text",
			"text":"### 2.3.1. Clients and Servers\n\n#### Overview\n\nIn a client-server model, the operating system is structured into multiple cooperating processes. These processes are categorized as either **clients** or **servers**:\n\n- **Clients**: These are the programs or devices that request services.\n- **Servers**: These are the programs or devices that provide the services.\n\nTypically, both clients and servers run on the same microkernel, and they operate as user processes. A single machine might run multiple clients, multiple servers, or a mix of both.\n\n#### Request/Reply Model\n\nThe client-server communication often uses a **simple, connectionless request/reply protocol**. Here’s how it works:\n\n1. **Client Request**: A client sends a request message to a server asking for a service (e.g., reading a file).\n2. **Server Response**: The server processes the request and sends back the data or an error message.\n\nThis interaction is depicted in the diagram below:\n\n```\n(a) Request/Reply Communication\nClient --> Request --> Server\nClient <-- Reply <-- Server\n```\n\n#### Advantages\n\n1. **Simplicity**: The request/reply protocol is straightforward. The client sends a request and receives a reply. There’s no need to establish or tear down a connection, which simplifies the process.\n\n2. **Efficiency**: The protocol stack is shorter. Here’s a breakdown of the layers involved:\n\n   - **Physical Layer**: Handles the physical transmission of data.\n   - **Data Link Layer**: Manages the data frames between machines.\n   - **Request/Reply Protocol**: Handles the actual sending and receiving of requests and replies.\n\n   The diagram for this simplified protocol stack is:\n\n   ```\n   (b) Simplified Protocol Stack\n   Physical Layer\n   Data Link Layer\n   Request/Reply Protocol\n   ```\n\n   Because the system skips the need for connection management and routing, the protocol stack is more efficient compared to connection-oriented models like OSI or TCP/IP.\n\n#### System Calls for Communication\n\nIn this simple model, communication services are typically reduced to two basic system calls:\n\n- **Sending a Message**: `send(dest, &mptr)`\n  - Sends a message to the process identified by `dest`.\n  - Blocks the caller until the message is sent.\n\n- **Receiving a Message**: `receive(addr, &mptr)`\n  - Blocks the caller until a message arrives.\n  - Copies the received message to the buffer pointed to by `mptr`.\n\nThese system calls handle the communication without needing complex session management. They simplify the process, making the system more efficient and easier to manage.\n\n#### Conclusion\n\nThe client-server model, using a simple request/reply protocol, provides an efficient and straightforward way to handle communication in distributed systems. By minimizing the protocol stack and avoiding complex connection management, it achieves simplicity and efficiency in data exchange.",
			"styleAttributes":{},
			"x":450,
			"y":3370,
			"width":1160,
			"height":920,
			"color":"1"
		},
		{
			"id":"f9f4403250cdfa71",
			"type":"file",
			"file":"source-images/Pasted image 20240903214832.png",
			"styleAttributes":{},
			"x":1810,
			"y":2330,
			"width":1515,
			"height":889
		},
		{
			"id":"c1b80adb9c2ecc20",
			"type":"file",
			"file":"source-images/Pasted image 20240903215813.png",
			"styleAttributes":{},
			"x":3090,
			"y":4195,
			"width":2040,
			"height":1083
		},
		{
			"id":"fa6ed361fc210b90",
			"type":"file",
			"file":"source-images/Pasted image 20240903214850.png",
			"styleAttributes":{},
			"x":3325,
			"y":1549,
			"width":1683,
			"height":682
		},
		{
			"id":"1523dbbe24d5d69f",
			"type":"file",
			"file":"source-images/Pasted image 20240903215907.png",
			"styleAttributes":{},
			"x":5450,
			"y":5730,
			"width":1800,
			"height":972,
			"color":"5"
		},
		{
			"id":"1349b49bf72982dc",
			"type":"file",
			"file":"source-images/Pasted image 20240903215748.png",
			"styleAttributes":{},
			"x":5450,
			"y":4290,
			"width":1648,
			"height":893
		},
		{
			"id":"0e6c15a8a6b48e59",
			"type":"file",
			"file":"source-images/Pasted image 20240903215711.png",
			"styleAttributes":{},
			"x":5250,
			"y":2970,
			"width":1728,
			"height":980
		},
		{
			"id":"00e1821355256944",
			"type":"text",
			"text":"**Basic RPC Operation**\n\nTo understand Remote Procedure Call (RPC), it's helpful to first grasp how a conventional local procedure call works. Here’s a breakdown of RPC operation:\n\n### **Conventional Local Procedure Call**\n\n1. **Stack Setup**: In a single machine setup, when a procedure is called (e.g., `count = read(fd, buf, nbytes);`), the stack frame is prepared:\n   - **Before Call**: The stack contains local variables for the calling procedure.\n   - **During Call**: Parameters are pushed onto the stack in reverse order. The procedure is then invoked.\n   - **After Call**: The procedure executes, returns a value, and the stack is cleaned up.\n\n   This setup allows the procedure to access and manipulate parameters as local variables. Value parameters are copied, while reference parameters (e.g., arrays) are passed by address.\n\n### **RPC Operation**\n\nRPC is designed to make remote procedure calls look like local ones, abstracting away the network communication details:\n\n1. **Client Call**: \n   - The client program calls a procedure (e.g., `read(fd, buf, nbytes)`) as if it were local.\n   - Instead of calling the actual procedure, this call is handled by a **client stub**. \n\n2. **Client Stub**:\n   - **Packing**: The client stub packs the parameters into a message.\n   - **Send**: It then uses the operating system’s kernel to send this message over the network to the server machine.\n\n3. **Server Reception**:\n   - **Receive**: The server's kernel receives the message and passes it to a **server stub** on the server machine.\n   - **Unpacking**: The server stub unpacks the parameters from the message.\n\n4. **Server Stub**:\n   - **Call**: The server stub then makes a local call to the actual server procedure with the unpacked parameters.\n   - **Execution**: The server procedure executes and produces a result.\n\n5. **Returning the Result**:\n   - **Pack Result**: The server stub packs the result into a message and sends it back to the client.\n   - **Send Back**: The server's kernel handles the sending of this message back to the client’s kernel.\n\n6. **Client Reception**:\n   - **Receive**: The client’s kernel receives the message and passes it to the client stub.\n   - **Unpacking**: The client stub unpacks the result and returns it to the client’s procedure.\n\n### **Steps Summarized**\n\n1. **Client Procedure** calls the client stub.\n2. **Client Stub** builds a message and sends it to the server.\n3. **Server Kernel** receives the message and hands it to the server stub.\n4. **Server Stub** unpacks the parameters and calls the server procedure.\n5. **Server Procedure** executes and returns the result.\n6. **Server Stub** packs the result and sends it back to the client.\n7. **Client Kernel** receives the result and delivers it to the client stub.\n8. **Client Stub** unpacks the result and returns it to the client procedure.\n\n### **Advantages of RPC**\n\n- **Transparency**: RPC makes remote procedure calls appear like local ones, abstracting the underlying network communication details.\n- **Ease of Use**: Developers can write distributed systems using familiar local procedure call semantics, simplifying the development process.\n\n### **Challenges in RPC**\n\n- **Complexity**: Although RPC hides network complexities, it introduces its own complexities in message passing, serialization, and error handling.\n- **Performance Overhead**: Network communication and data serialization/deserialization introduce additional overhead compared to local procedure calls.\n\nOverall, RPC aims to make distributed computing more intuitive by mimicking local procedure calls, while managing the intricacies of remote communication and execution.",
			"styleAttributes":{},
			"x":-770,
			"y":6750,
			"width":1180,
			"height":520,
			"color":"2"
		},
		{
			"id":"8686ff98cb236073",
			"type":"text",
			"text":"### Parameter Passing in RPC Systems\n\nIn Remote Procedure Call (RPC) systems, parameter passing involves several steps and challenges, particularly related to different machine architectures and data representations. This section discusses how parameters are packed into messages, transferred, and unpacked, addressing issues such as data format inconsistencies and pointer handling.\n\n#### Basic Operation\n\nWhen a client calls a remote procedure, such as `sum(i, j)`, the client stub takes the parameters `i` and `j`, packs them into a message, and sends it to the server stub. This message also includes the procedure identifier to specify which remote procedure to invoke. Upon receiving the message, the server stub determines the correct procedure to call and then executes it with the received parameters. After the server completes the operation, the server stub packs the result into a message and sends it back to the client stub, which then unpacks and returns the result to the client procedure.\n\n#### Issues with Data Representation\n\nWhen different machines are involved, discrepancies in data representation can arise. For example, IBM mainframes use EBCDIC for character encoding, while personal computers use ASCII. Additionally, integer representations (e.g., one’s complement vs. Two’s complement) and floating-point formats can differ between machines. Byte order (endianness) also varies: Intel processors use little-endian format, while others like the SPARC use big-endian.\n\n**Example: Byte Order Issue**\n\nConsider a scenario where a message containing an integer and a string is sent from an Intel 486 machine to a SPARC machine. On the Intel 486, the integer `5` and the string \"JILL\" are packed with little-endian byte order. When received by the SPARC (which uses big-endian format), the byte order is reversed, leading to incorrect interpretations of the integer and string. \n\nTo resolve such issues, a standard or canonical form can be used for data representation. For instance, agreeing on a fixed byte order and format ensures that all systems interpret the data consistently. \n\n#### Handling Different Data Formats\n\nTo handle various data formats, RPC systems may use two main strategies:\n1. **Standard Representation**: All systems convert data to a canonical form (e.g., little-endian) before sending. This approach ensures consistency but may involve unnecessary conversions if both client and server use the same format.\n2. **Native Format**: The client sends data in its native format along with a format identifier. The server checks this identifier and performs conversions only if necessary.\n\n#### Handling Pointers\n\nPointers are particularly challenging in RPC systems because their addresses are meaningful only within the context of a specific machine's memory. Directly passing pointer values across machines is impractical as the addresses are not valid in the remote machine's address space.\n\n**Solution: Copy/Restore Mechanism**\n\nOne common approach is to avoid passing pointers directly. Instead, the data pointed to is copied into the message and sent to the server. The server processes this data and sends it back if necessary. This method is referred to as \"copy/restore\" rather than true call-by-reference.\n\n**Optimization**\n\nIf the formal specification of a procedure indicates whether parameters are input or output, unnecessary copying can be avoided. For example, if a buffer is used only for output, it need not be copied from the client to the server and back.\n\n#### Generation of Stubs\n\nStub procedures, which handle marshalling and unmarshalling of data, are typically generated automatically based on the formal specification of the server procedure. This approach ensures consistency and reduces errors, as both client and server stubs adhere to the same message format.\n\n#### Conclusion\n\nHandling parameter passing in RPC systems involves careful consideration of data formats and pointer mechanisms. While data representation issues can be addressed through standardization or format-specific handling, pointer management generally relies on copying data and optimizing based on parameter usage. Automated stub generation helps maintain consistency and reduce programming errors.\n\n### **Parameter Passing in RPC**\n\nWhen using Remote Procedure Calls (RPC), parameter passing is a critical aspect, as it involves sending and receiving data between different machines. This process is known as parameter marshaling. Here’s a detailed look at how parameter passing works in RPC systems:\n\n---\n\n#### **1. Marshaling Parameters**\n\n**Marshaling** is the process of packing parameters into a message for transmission. For instance, consider a simple remote procedure `sum(i, j)` that adds two integers. \n\n1. **Client Side**: \n   - **Call**: `n = sum(4, 7);`\n   - **Client Stub**: Packs parameters `4` and `7` into a message along with the procedure identifier.\n   \n2. **Server Side**:\n   - **Message Handling**: The server stub receives the message, unpacks it, and invokes the appropriate server procedure based on the identifier.\n\n   ![Example of RPC Parameter Passing](https://example.com/image) *(Figure not included in text)*\n\n---\n\n#### **2. Handling Different Data Representations**\n\n**Data Representation Issues**:\n1. **Character Encoding**:\n   - Different systems may use different character encodings (e.g., ASCII vs. EBCDIC). This mismatch can cause incorrect interpretation of data.\n\n2. **Integer Representation**:\n   - Systems might use different representations for integers (e.g., 1’s complement vs. 2’s complement).\n\n3. **Endianness**:\n   - Systems might use different byte orders (little-endian vs. Big-endian). For example:\n     - **Intel 486** uses little-endian.\n     - **SPARC** uses big-endian.\n\n   This difference can lead to incorrect data interpretation if not handled properly.\n\n   ![Byte Ordering Example](https://example.com/image) *(Figure not included in text)*\n\n**Example Issue**:\n- An integer and a string packed on an Intel 486 might be misinterpreted on a SPARC due to endianness differences. \n\n---\n\n#### **3. Standardizing Data Formats**\n\n**Canonical Form**:\n1. **Network Standards**:\n   - Establish a standard format for data types (e.g., ASCII for characters, IEEE format for floating-point numbers) to ensure compatibility across different systems.\n\n2. **Conversion**:\n   - Systems should convert their internal representations to this standard format before sending data, and convert back upon receiving.\n\n   **Pros**:\n   - Avoids confusion and ensures compatibility.\n\n   **Cons**:\n   - Can be inefficient due to unnecessary conversions.\n\n**Alternative Approach**:\n1. **Native Formats**:\n   - Clients send data in their native format, including a format indicator in the message.\n   - The server checks the format and converts if necessary.\n\n   **Pros**:\n   - Reduces conversion overhead if client and server use the same format.\n   - Only performs conversions when required.\n\n---\n\n#### **4. Passing Complex Data Types**\n\n**Pointers and References**:\n1. **Challenges**:\n   - Pointers are meaningful only in the local address space and cannot be directly passed to a remote server.\n\n2. **Solutions**:\n   - **Copy/Restore**: Copy the data referenced by the pointer into the message, and the server operates on this copy. The server sends the modified data back.\n\n   **Optimizations**:\n   - If the data is only input or output, avoid unnecessary copying.\n\n3. **Pointer Handling**:\n   - **Special Techniques**: For complex data structures, some systems may use messages to request data fetching or storing.\n\n   **Cons**:\n   - Can be highly inefficient for large or complex data structures.\n\n---\n\n#### **5. Stub Generation**\n\n**Automatic Stub Generation**:\n1. **Tools**:\n   - RPC systems often use a formal specification of procedures to automatically generate client and server stubs. \n\n2. **Benefits**:\n   - Reduces manual coding errors.\n   - Ensures consistency in data representation and parameter handling.\n\n---\n\n### **Summary**\n\nParameter passing in RPC involves marshaling parameters into messages, handling various data representations, and dealing with complex data types. Standardizing data formats and automating stub generation are crucial for ensuring smooth communication and minimizing errors in distributed systems.",
			"styleAttributes":{},
			"x":-770,
			"y":7330,
			"width":1180,
			"height":720,
			"color":"2"
		},
		{
			"id":"eea9a5f4a956d901",
			"type":"text",
			"text":"### Dynamic Binding\n\nDynamic binding is a technique used to locate and connect clients with servers in a flexible and scalable manner, avoiding the issues associated with hardcoded server addresses. It relies on a service called a binder to manage the mapping between clients and servers.\n\n#### Server Specification\n\nEach server provides a formal specification that includes:\n- **Server Name**: The unique name of the server.\n- **Version Number**: The version of the server's interface.\n- **Procedure List**: The procedures available on the server along with parameter types and directions (in, out, in-out).\n\nFor example, a server specification might include procedures like `read`, `write`, `create`, and `delete`, with detailed information about parameter types and their directions.\n\n#### Registering with the Binder\n\nWhen a server starts, it registers itself with the binder by:\n- **Sending its Name, Version, Handle, and a Unique Identifier**: The handle is used to locate the server and can be an address or identifier like an IP address. The unique identifier helps in distinguishing between different instances of the same server.\n- **Optionally Providing Additional Information**: Such as authentication details.\n\n#### Client-Server Binding Process\n\n1. **Client Request**: When a client needs to call a remote procedure, it first checks if it is bound to a server. If not, it sends a request to the binder to find a server with the required interface.\n2. **Binder Lookup**: The binder checks for servers that have registered the required interface version. If a suitable server is found, the binder provides the client stub with the server's handle and unique identifier.\n3. **Message Handling**: The client stub uses the handle to send the request to the server. The server’s kernel uses the unique identifier to route the message to the correct server instance if multiple servers are present.\n\n#### Advantages\n\n- **Flexibility**: Servers can move, replicate, or change interfaces without requiring client modifications.\n- **Load Balancing**: The binder can distribute requests across multiple servers to balance load.\n- **Fault Tolerance**: The binder can detect and deregister failed servers, improving system reliability.\n- **Authentication and Versioning**: The binder can enforce access controls and ensure clients use compatible versions of the server interface.\n\n#### Disadvantages\n\n- **Overhead**: Registering and looking up services introduces additional latency, which can impact performance, especially in systems with many short-lived client processes.\n- **Potential Bottleneck**: In large distributed systems, the binder can become a bottleneck, necessitating multiple binders to handle the load efficiently.\n\nDynamic binding thus provides a robust mechanism for managing client-server interactions in a distributed environment, balancing flexibility with the need for efficient service discovery and connection management.",
			"styleAttributes":{},
			"x":-770,
			"y":8130,
			"width":1180,
			"height":660,
			"color":"2"
		},
		{
			"id":"6fa7b486e0857436",
			"type":"text",
			"text":"### RPC Semantics in the Presence of Failures\n\nRemote Procedure Call (RPC) aims to make remote calls as seamless as local ones. However, various failures can disrupt this seamlessness. Here's how different types of failures impact RPC systems and potential solutions for each:\n\n#### Types of Failures\n\n1. **Client Cannot Locate the Server**\n   - **Issue**: The client fails to find a suitable server due to the server being down or version mismatches.\n   - **Solution**: Use error codes (like `-1`), exception handling, or signal handlers to report errors. Some systems might raise exceptions or use signals to indicate issues, but this can break the transparency of RPC.\n\n2. **Lost Request Messages**\n   - **Issue**: The request message sent by the client may be lost.\n   - **Solution**: Implement timeouts and retransmissions. The client kernel starts a timer when sending a request. If no reply is received before the timer expires, the message is sent again. If messages are lost frequently, it might be mistaken for a server outage.\n\n3. **Lost Reply Messages**\n   - **Issue**: The reply from the server to the client may be lost, leading to confusion about whether the request was processed.\n   - **Solution**:\n     - **Idempotent Requests**: Ensure that requests can be repeated safely without adverse effects (e.g., retrieving file data).\n     - **Sequence Numbers**: Assign sequence numbers to each request so the server can differentiate between new and retransmitted requests.\n     - **Request Distinction**: Use bits in the message header to indicate whether a request is new or a retransmission.\n\n4. **Server Crashes**\n   - **Issue**: The server might crash after receiving a request but before sending a reply, or before even processing the request.\n   - **Solution**:\n     - **At Least Once Semantics**: Retry the request until a reply is received or a timeout occurs. This ensures the request is executed at least once but may be repeated.\n     - **At Most Once Semantics**: Ensure the request is executed at most once, or not at all, by reporting failure immediately if no reply is received.\n     - **No Guarantees**: No promises about the number of executions, simplifying implementation but providing no assurance to the client.\n\n5. **Client Crashes**\n   - **Issue**: If a client crashes after sending a request but before receiving a reply, it can create \"orphaned\" computations that waste resources or cause confusion.\n   - **Solution**:\n     - **Extermination**: Log each RPC on disk. After rebooting, check the log to terminate orphans. This approach can be costly and may not work in network partitions.\n     - **Reincarnation**: Use time epochs to mark the start of new periods. Broadcast epoch updates to terminate remote computations. Orphans with outdated epoch numbers are detected and handled.\n     - **Gentle Reincarnation**: Only terminate remote computations if their owner cannot be located, reducing unnecessary terminations.\n     - **Expiration**: Assign a time limit for each RPC. If it doesn’t complete in time, it must request more time, or the server waits a sufficient period before rebooting to ensure all orphans are gone.\n\n#### Summary\n\nEach type of RPC failure requires specific handling strategies to maintain system reliability and performance. Addressing these issues involves balancing complexity, efficiency, and the guarantees provided to clients.",
			"styleAttributes":{},
			"x":-770,
			"y":8850,
			"width":1180,
			"height":840,
			"color":"2"
		},
		{
			"id":"c91c70b64a43cec1",
			"type":"text",
			"text":"### Implementation Issues in RPC Systems\n\n#### RPC Protocols\n\n1. **Protocol Choice:**\n   - **Connection-Oriented vs. Connectionless:**\n     - **Connection-Oriented**: Establishes a connection between client and server, simplifying communication (e.g., TCP). This is beneficial for wide-area networks but can introduce performance overhead on LANs where connectionless protocols (e.g., UDP) may be more efficient.\n     - **Connectionless**: No need to establish a persistent connection, which reduces overhead (e.g., UDP). This is often preferred in LAN environments due to its simplicity and reduced overhead.\n\n2. **Standard vs. Custom Protocols:**\n   - **Standard Protocols**: IP and UDP are commonly used because they are well-supported and integrate easily with existing systems. However, they have overhead (e.g., checksums, fragmentation) that can affect performance.\n   - **Custom Protocols**: Can be designed to minimize overhead and optimize performance for specific needs, though they require significant development effort and are less universally compatible.\n\n3. **Packet and Message Length:**\n   - **Single Large RPC vs. Multiple Small RPCs:** Sending large messages in a single RPC is generally more efficient than breaking it into smaller RPCs due to reduced overhead.\n   - **Network Limits:** Networks and protocols may impose limits on packet size, affecting how messages are transmitted and possibly requiring fragmentation.\n\n#### Acknowledgements\n\n1. **Stop-and-Wait Protocol:**\n   - Sends a packet and waits for an acknowledgment before sending the next one. This ensures reliability but can be slow due to waiting times.\n\n2. **Blast Protocol:**\n   - Sends all packets without waiting for acknowledgments for each one. The entire message is acknowledged at once. This method is more efficient but requires handling of lost packets and flow control issues.\n\n3. **Flow Control and Overrun Errors:**\n   - **Stop-and-Wait**: Prevents overrun errors as it waits for an acknowledgment before sending the next packet.\n   - **Blast Protocol**: May suffer from overrun errors if the receiver's buffer cannot handle back-to-back packets. Techniques like busy waiting or adding delays can help mitigate these issues.\n\n#### Critical Path\n\n- **Definition:** The sequence of steps executed during an RPC call from the client to the server and back.\n- **Steps Involved:**\n  1. **Client Stub:** Acquires buffer, marshals parameters, and prepares the message.\n  2. **Kernel:** Handles context switch, copies message to kernel space, and sends it to the network.\n  3. **Server Side:** Receives message, processes it, and sends a reply back.\n\n- **Performance Bottlenecks:** Analysis often shows that the most time-consuming steps can be:\n  - Context switches\n  - Interrupt handling\n  - Network transmission time\n  - Copying operations\n\n#### Copying\n\n1. **Impact of Copying:**\n   - Multiple copies of data between user and kernel space can significantly impact performance. The more copies needed, the slower the RPC.\n   - **Scatter-Gather:** Efficient method to minimize copying by allowing direct assembly or disassembly of packets by the network chip.\n\n2. **Optimizations:**\n   - **Mapping Buffers:** Using virtual memory to map buffers directly into the address space of the receiving process can reduce copying. However, this requires careful alignment and management.\n\n### Summary\n\n- **Protocol Choice:** Affects overhead and efficiency, with trade-offs between connection-oriented and connectionless protocols.\n- **Acknowledgements:** Different methods have varying impacts on reliability and performance.\n- **Critical Path:** Identifies performance bottlenecks in RPC execution.\n- **Copying:** Minimizing copying through techniques like scatter-gather and buffer mapping can improve performance.\n\nThese considerations are essential for optimizing the performance of RPC systems and ensuring efficient communication in distributed environments.",
			"styleAttributes":{},
			"x":-770,
			"y":9790,
			"width":1180,
			"height":1240
		},
		{
			"id":"937047e4f09245c8",
			"type":"text",
			"text":"# **Remote Procedure Call (RPC)** is a powerful concept introduced to improve the ease of distributed computing by making it resemble centralized computing. Here’s a detailed explanation:\n\n### **Concept**\n\nRPC allows a program on one machine (the client) to call a procedure or function that is executed on another machine (the server). The key idea is that from the perspective of the client, calling a remote procedure should be as straightforward as calling a local procedure. The complexity of network communication is abstracted away, making distributed systems easier to program and use.\n\n### **How It Works**\n\n1. **Procedure Call**: The client makes a procedure call as if it were local. The RPC system intercepts this call and handles the details of communication with the server.\n   \n2. **Execution**: The procedure call is sent to the server, where the corresponding procedure is executed.\n\n3. **Result**: After execution, the result is sent back to the client, and the client’s execution resumes as if the procedure had been executed locally.\n\n### **Challenges and Complications**\n\n1. **Address Spaces**: Since the client and server are on different machines, they operate in different memory address spaces. This separation requires mechanisms to serialize (convert to a format suitable for transmission) and deserialize (convert back) the parameters and results.\n\n2. **Parameter and Result Passing**: Passing parameters and results between different machines can be complex, especially if the machines have different architectures, data representations, or byte orders. This requires careful handling to ensure data consistency and correctness.\n\n3. **Machine Failures**: Both the client and server machines can experience failures. RPC systems need to handle various failure scenarios, including network failures, server crashes, and partial failures where one machine fails but the other continues to run.\n\n### **Implementation**\n\nThe implementation of RPC involves several key components:\n\n1. **Stub Generation**: RPC systems use stubs (proxies) to handle communication between the client and server. The client-side stub appears as the actual procedure to the client, while the server-side stub handles the execution of the procedure on the server.\n\n2. **Marshalling and Unmarshalling**: This process involves converting procedure parameters and results into a format that can be transmitted over the network (marshalling) and then converting them back into their original format on the receiving end (unmarshalling).\n\n3. **Communication Protocol**: RPC systems use various protocols for communication, such as TCP or UDP, to ensure reliable transmission of procedure calls and results.\n\n4. **Error Handling**: Robust RPC implementations include mechanisms for handling errors, such as retries, timeouts, and acknowledgment of receipt.\n\n### **Strengths**\n\n- **Simplicity**: RPC abstracts the complexities of network communication, making distributed systems easier to develop and use.\n- **Transparency**: It allows developers to write distributed applications without needing to handle low-level network communication explicitly.\n- **Integration**: RPC is widely supported and integrated into many distributed operating systems and applications.\n\n### **Weaknesses**\n\n- **Performance Overhead**: The process of marshalling and unmarshalling data, along with network communication, introduces overhead compared to local procedure calls.\n- **Error Handling**: Handling network-related errors and machine failures can be complex and requires additional mechanisms.\n- **Cross-platform Issues**: Differences in machine architectures and data representations can complicate the implementation of RPC systems.\n\nIn summary, RPC simplifies distributed computing by making remote procedure calls appear like local calls, abstracting away the complexities of network communication. Despite its advantages, it also faces challenges related to parameter passing, machine failures, and performance overhead.",
			"styleAttributes":{},
			"x":-2010,
			"y":6990,
			"width":860,
			"height":780,
			"color":"1"
		},
		{
			"id":"b238264452fc35ed",
			"type":"text",
			"text":"Certainly! Here’s a detailed explanation of each layer in the OSI model:\n\n### The Physical Layer\n\nThe Physical Layer is the foundation of the OSI model and is responsible for the actual transmission of binary data—0 s and 1 s—over a physical medium. It deals with the technical aspects of sending and receiving raw bit streams over various types of transmission media, including electrical cables, optical fibers, or radio waves. This layer defines the hardware elements involved in the communication process, including voltage levels used to represent binary values (0 s and 1 s), the data rate (measured in bits per second), and the physical connectors and cables used to connect devices. For example, the RS-232-C standard specifies serial communication lines, including the connector shape and pin configuration, to ensure that a transmitted 0 bit is accurately received as a 0 bit and not misinterpreted due to variations in signal strength or noise.\n\n### The Data Link Layer\n\nThe Data Link Layer builds on the Physical Layer by addressing the need for error detection and correction in data transmission. While the Physical Layer transmits raw bits, the Data Link Layer organizes these bits into frames—structured units of data that include not just the raw payload but also headers and trailers for control purposes. Each frame is marked with special bit patterns at the beginning and end to define its boundaries. To ensure data integrity, the Data Link Layer calculates and attaches a checksum to each frame. Upon receiving a frame, the receiver recalculates the checksum and compares it with the transmitted value. If they match, the frame is considered error-free and accepted; if not, the receiver requests a retransmission. This layer also handles frame sequencing, which involves assigning unique sequence numbers to each frame to keep track of the data being sent and ensure that all frames are correctly assembled and processed. This mechanism helps manage retransmissions and handles communication errors effectively.\n\n### The Network Layer\n\nThe Network Layer is crucial for managing data transmission across multiple networks, which involves routing and forwarding data packets from the sender to the receiver. Unlike Local Area Networks (LANs), where data transmission is straightforward, wide-area networks (WANs) consist of numerous interconnected nodes and paths. The Network Layer’s primary task is routing, which determines the best path for data to travel from the source to the destination. This process involves calculating routes based on various factors, including traffic load and network congestion. Routing decisions can be influenced by algorithms that adapt to current network conditions or rely on long-term averages. The Network Layer supports both connection-oriented and connectionless communication. For instance, X.25 is a connection-oriented protocol that sets up a connection before data transfer and maintains it for the duration of the communication. On the other hand, IP (Internet Protocol) operates in a connectionless manner, sending packets independently without pre-establishing a connection or maintaining a dedicated path.\n\n### The Transport Layer\n\nThe Transport Layer is responsible for providing end-to-end communication services and ensuring that data is reliably transmitted between systems. It manages the segmentation of data from the session layer into smaller packets, assigns sequence numbers to these packets, and then sends them across the network. This layer also handles the reassembly of packets at the destination to reconstruct the original message. It is designed to manage issues such as packet loss, duplication, and ordering. Reliable transport protocols, such as TCP (Transmission Control Protocol), ensure that packets are delivered correctly and in the correct order, providing mechanisms for error detection, retransmission, and flow control. TCP guarantees that all packets reach the receiver in the correct sequence and without loss. Conversely, UDP (User Datagram Protocol) provides a connectionless service, offering minimal overhead and faster transmission but without guarantees of reliability or order. The Transport Layer protocols ensure that data transfer meets the needs of different applications, whether they require guaranteed delivery or are tolerant of some degree of data loss.\n\n### The Session Layer\n\nThe Session Layer extends the functionalities of the Transport Layer by providing mechanisms for managing sessions or dialogues between applications. It is responsible for establishing, maintaining, and terminating connections between two communicating systems. This layer facilitates dialog control, which determines which party is currently communicating and manages the session’s overall state. It also provides synchronization features, allowing applications to set checkpoints during long data transfers. If a failure occurs, the session can resume from the last checkpoint rather than starting over from scratch. Although these capabilities are crucial for some applications, the Session Layer is often less emphasized in modern networking environments, and its functions are sometimes integrated into other layers or handled by application-specific protocols.\n\n### The Presentation Layer\n\nThe Presentation Layer focuses on the syntax and semantics of the data being transmitted. It is responsible for translating, encrypting, and compressing data to ensure that it is understandable and usable by the receiving system. This layer converts data into a format that the application layer can interpret correctly, handling tasks such as data encoding, conversion between different character sets, and data encryption. For example, it may translate data from EBCDIC to ASCII or compress data to reduce transmission time. The Presentation Layer ensures that data structures, formats, and representations are consistent between systems with different internal data formats, facilitating interoperability and making data exchange more seamless.\n\n### The Application Layer\n\nThe Application Layer is the topmost layer in the OSI model and serves as the interface between the user applications and the network. It provides various protocols for network services and application-specific functions, such as electronic mail, file transfer, and remote terminal access. This layer encompasses protocols like HTTP (HyperText Transfer Protocol) for web browsing, FTP (File Transfer Protocol) for transferring files, and SMTP (Simple Mail Transfer Protocol) for email. It directly interacts with end-user applications and ensures that they can communicate effectively over the network. Although the specific protocols and services provided at this layer can vary, they are essential for enabling users to access networked resources and applications efficiently. \n\nIn summary, the OSI model’s layered approach helps manage the complexity of network communication by dividing it into distinct, manageable parts, each with its own set of responsibilities and protocols. This structured methodology facilitates the design, implementation, and troubleshooting of network systems by providing a clear framework for understanding and addressing various aspects of data transmission.\n### Key Concepts in Layered Protocols\n\n1. **Layer Independence**: Each layer operates independently of the others. Changes in one layer do not necessarily affect other layers, making the system modular and easier to manage. For example, switching from postal mail to FAX for sending orders involves changes only at the physical and data link layers, while the content of the orders (handled by the application layer) remains unaffected.\n\n2. **Encapsulation**: Each layer adds its own header (and sometimes trailer) to the data it receives from the layer above. This process is known as encapsulation. For instance, in Fig. 2-2, you can see how various headers and trailers are added as the data moves from the application layer down to the physical layer and how it is formatted for transmission.\n\n3. **Protocols and Interfaces**: The OSI model defines the protocol suite used at each layer and the interface between layers. Each layer's protocol is responsible for a specific function, and its interface specifies how it interacts with the layer above and below it.\n\n4. **Connection-Oriented vs. Connectionless**: \n   - **Connection-Oriented Protocols**: Require establishing a connection before data exchange (e.g., TCP). This approach involves a handshake to set up the connection and teardown once communication is complete.\n   - **Connectionless Protocols**: Allow data to be sent without establishing a connection beforehand (e.g., UDP). There is no guarantee of delivery, order, or error checking.\n",
			"styleAttributes":{},
			"x":479,
			"y":2330,
			"width":1200,
			"height":640,
			"color":"1"
		},
		{
			"id":"0634f8ccaed92e74",
			"type":"file",
			"file":"source-images/Pasted image 20240903213822.png",
			"styleAttributes":{},
			"x":479,
			"y":1530,
			"width":1198,
			"height":718
		},
		{
			"id":"abce02916ead6210",
			"type":"file",
			"file":"source-images/Pasted image 20240903235445.png",
			"styleAttributes":{},
			"x":-1879,
			"y":4490,
			"width":2169,
			"height":800
		},
		{
			"id":"0b8071fa6382638c",
			"type":"text",
			"text":"## 2.3. THE CLIENT-SERVER MODEL\n\n### Overview\n\nWhile ATM (Asynchronous Transfer Mode) networks promise significant advances in networking, their high cost makes them impractical for many current applications. Consequently, conventional networking methods and their associated protocols become more relevant. A common approach to structuring distributed systems is the use of layered protocols, like those defined by the OSI model. These protocols provide a framework for transmitting data across networks, but they come with their own set of issues.\n\n### Limitations of Layered Protocols\n\n#### Protocol Overhead\n\nLayered protocols involve a series of processing stages, each adding and then removing its own header information as data travels from the sender to the receiver. This process introduces overhead because every message must pass through multiple layers of header generation and examination. \n\nFor wide-area networks (WANs), where the transmission rate can be relatively low (e.g., 64 Kbps), the primary limitation is the bandwidth of the communication lines. Despite the overhead, the CPU power is usually sufficient to handle the protocol processing without significantly affecting performance. Thus, protocols like OSI or TCP/IP can be employed without substantial performance degradation in WAN settings.\n\n#### Impact on LANs\n\nIn contrast, local area networks (LANs) often operate at much higher speeds compared to WANs. In such high-speed environments, the protocol overhead becomes more pronounced. The CPU time required for protocol processing can become a bottleneck, reducing the effective throughput of the network. Consequently, many LAN-based distributed systems either avoid using layered protocols or employ only a subset of the full protocol stack to minimize overhead.\n\n### The Client-Server Model\n\nThe OSI model and similar layered approaches primarily address data transmission aspects—ensuring bits reach their destination correctly and in order. However, these models do not offer guidance on the structural organization of distributed systems. To address this, the client-server model provides an alternative approach:\n\n- **Client**: A client is an entity that requests resources or services from another entity. It initiates communication and typically interacts with the server to access data or services.\n\n- **Server**: A server provides resources or services to clients. It listens for incoming requests, processes them, and returns the appropriate responses.\n\nThe client-server model focuses on the roles and interactions between clients and servers rather than on the detailed mechanics of data transmission. This model simplifies the design and management of distributed systems by clearly delineating responsibilities and interactions between different components.\n\n### Advantages of the Client-Server Model\n\n1. **Simplified Communication**: By separating the roles of clients and servers, the client-server model simplifies communication patterns and interactions in distributed systems.\n\n2. **Scalability**: The model allows for scalable system design, where multiple clients can interact with a single server or multiple servers can be employed to handle client requests efficiently.\n\n3. **Flexibility**: It provides flexibility in terms of protocol and implementation, enabling systems to optimize performance and overhead based on specific needs and constraints.\n\n4. **Maintenance and Management**: The clear separation of roles makes it easier to manage and maintain the system, as updates or changes can be made on the server-side without affecting client operations directly.\n\n### Conclusion\n\nWhile layered protocols like OSI provide a structured approach to data transmission, they may not always be the most efficient solution, particularly in high-speed LAN environments where overhead can be a concern. The client-server model offers an alternative framework that focuses on the organizational aspects of distributed systems, providing a more effective way to manage and structure interactions between clients and servers. This model addresses the need for a clear, efficient approach to distributed system design beyond just data transmission.",
			"styleAttributes":{},
			"x":-1470,
			"y":3450,
			"width":1760,
			"height":920,
			"color":"1"
		},
		{
			"id":"38983dab07db9e1e",
			"type":"file",
			"file":"source-images/Pasted image 20240903200323.png",
			"styleAttributes":{},
			"x":-950,
			"y":2510,
			"width":1123,
			"height":620,
			"color":"2"
		},
		{
			"id":"746c5f7d7a19a622",
			"type":"file",
			"file":"source-images/Pasted image 20240903214508.png",
			"styleAttributes":{},
			"x":1770,
			"y":1530,
			"width":1327,
			"height":720
		},
		{
			"id":"caa0079a47d0fa42",
			"type":"text",
			"text":"In distributed systems, interprocess communication (IPC) is fundamentally different from that in uniprocessor systems due to the absence of shared memory. Here’s an overview of the key concepts and issues related to IPC in distributed systems, as well as the specific models and techniques used to manage communication.\n\n### Key Differences in IPC: Distributed Systems vs. Uniprocessor Systems\n\n1. **Shared Memory vs. Message Passing:**\n    \n    - **Uniprocessor Systems:** IPC often relies on shared memory where multiple processes can read and write to a common memory location. Examples include semaphores for synchronization and shared buffers for producer-consumer problems.\n    - **Distributed Systems:** There is no shared memory between processes running on different machines. IPC is achieved through message passing, where processes exchange messages over a network.\n2. **Protocol Adherence:**\n    \n    - In distributed systems, processes must adhere to well-defined protocols for communication. These protocols ensure that messages are formatted, transmitted, and received correctly despite the lack of shared memory.",
			"styleAttributes":{},
			"x":-908,
			"y":2134,
			"width":1040,
			"height":270,
			"color":"1"
		},
		{
			"id":"9096815183808072",
			"type":"file",
			"file":"source-images/Pasted image 20240921190519.png",
			"styleAttributes":{},
			"x":-1262,
			"y":-5137,
			"width":708,
			"height":751
		},
		{
			"id":"9d57e89998b35402",
			"type":"file",
			"file":"source-images/Pasted image 20240921191935.png",
			"styleAttributes":{},
			"x":-1212,
			"y":-3150,
			"width":1033,
			"height":733
		},
		{
			"id":"e4cad31ea0e659fb",
			"type":"file",
			"file":"source-images/Pasted image 20240921190611.png",
			"styleAttributes":{},
			"x":-2594,
			"y":-5720,
			"width":1247,
			"height":461
		},
		{
			"id":"8ceb5fe2d582afa6",
			"type":"text",
			"text":"The text explains the **OSI (Open Systems Interconnection)** model's seven layers, describing each one's role in data transmission across a network. Here's an overview of each layer:\n\n### 1. Physical Layer\n- **Purpose**: The lowest layer is responsible for the actual transmission of raw data (binary bits) over a physical medium, like cables.\n- **Key Concerns**: It manages how 0 s and 1 s are represented (e.g., voltage levels), the transmission speed (bits per second), and whether data can flow in both directions at the same time.\n- **Standardization**: It focuses on ensuring that physical components (like connectors) and transmission standards (e.g., RS-232-C for serial communication) are uniform, ensuring data is correctly transmitted.\n\n### 2. Data Link Layer\n- **Purpose**: Ensures error-free data transfer between two directly connected nodes by organizing bits into frames.\n- **Key Functions**:\n  - **Error Detection/Correction**: It adds checksums to frames, detects errors, and retransmits data if necessary.\n  - **Framing**: It groups bits into frames for better control over the data.\n  - **Communication**: The sender and receiver exchange control messages to ensure proper delivery.\n- **Example**: The provided example illustrates how errors in data transmission (e.g., corrupted messages) are handled by retransmitting data upon request.\n\n### 3. Network Layer\n- **Purpose**: Responsible for routing data across multiple interconnected networks (wide-area networks or WANs).\n- **Key Functions**:\n  - **Routing**: Determines the best path for data packets based on factors like traffic and delays.\n  - **Connection-oriented vs. Connectionless**: \n    - **X.25** (connection-oriented) establishes a route before transmission.\n    - **IP (Internet Protocol)** (connectionless) sends each packet independently without establishing a dedicated path.\n\n### 4. Transport Layer\n- **Purpose**: Provides reliable data transfer between two systems.\n- **Key Functions**:\n  - **Error Handling**: It breaks messages into packets, assigns sequence numbers, and ensures all packets are delivered in the correct order.\n  - **Connection-oriented protocols**: \n    - **TCP (Transmission Control Protocol)** ensures data is reliably delivered.\n    - **UDP (User Datagram Protocol)**, a simpler, connectionless protocol, is used for tasks that do not require guaranteed delivery.\n\n### 5. Session Layer\n- **Purpose**: Manages sessions or dialogues between two systems.\n- **Key Functions**:\n  - **Dialog Control**: It keeps track of which side is sending data at a given time.\n  - **Synchronization**: Allows users to insert checkpoints in data transfers, so if there’s an interruption, they can resume from the last checkpoint instead of starting over.\n- **Note**: This layer is rarely implemented in practice.\n\n### 6. Presentation Layer\n- **Purpose**: Ensures that data is in a readable format for both the sender and receiver, regardless of their system's internal data representation.\n- **Key Functions**:\n  - **Data Translation**: Defines formats for data types like text, numbers, and graphics.\n  - **Data Encryption/Decryption**: Handles any transformations to ensure the data is meaningful (e.g., converting between different encoding schemes).\n\n### 7. Application Layer\n- **Purpose**: The top layer, focused on providing network services directly to applications (like email or file transfer).\n- **Key Examples**:\n  - **X.400**: An electronic mail protocol.\n  - **X.500**: A directory service protocol.\n  \nOverall, these seven layers enable modular and standardized communication in distributed systems, ensuring data is transmitted efficiently, reliably, and meaningfully from one device to another.",
			"styleAttributes":{},
			"x":-2594,
			"y":-4156,
			"width":1248,
			"height":914,
			"color":"1"
		},
		{
			"id":"e3f958b1644cc021",
			"type":"text",
			"text":"Asynchronous Transfer Mode (ATM) is a high-performance networking standard designed to support a wide range of data traffic, including voice, video, and data, by transmitting fixed-size cells over virtual circuits. This technology emerged in the late 1980 s, driven by the need to address the limitations of traditional circuit-switched and packet-switched networks, particularly the challenge of managing both smooth, low-bandwidth voice traffic and bursty, high-bandwidth data traffic.\n\n### 2.2.1 What Is Asynchronous Transfer Mode?\n\nATM operates by establishing a connection between sender and receiver (s) via a virtual circuit. During connection setup, the network determines the route and stores the routing information in the switches. Data is then transmitted in small, fixed-size units called cells, which allows for fast and efficient switching, making ATM suitable for a variety of traffic types. The connection is terminated when no longer needed, and routing information is cleared.\n\n#### Key Features of ATM:\n- **Fixed-size cells:** Cells are 53 bytes in size, which enables faster switching compared to variable-sized packets used in traditional packet-switched networks. This eliminates delays caused by large packets occupying bandwidth.\n- **Virtual circuits:** ATM uses pre-established routes, which improve the efficiency of transmitting both voice and data, and support multicasting, needed for applications like broadcast television.\n- **Single network for multiple services:** ATM can handle voice, data, video, and other types of traffic in a single network, reducing costs and complexity. This technology allows for integrated services, including video conferencing, video-on-demand, and high-speed data access.\n  \nATM offers significant flexibility and performance improvements, making it a promising technology for future distributed systems and large-scale communications networks. However, ATM’s layered architecture has some differences from the OSI model, specifically in the roles of its lower layers:\n- **Physical Layer:** Manages the transmission of cells over the network (similar to OSI Layer 1).\n- **ATM Layer:** Handles cell transport and routing (covering OSI Layer 2 and part of Layer 3) but does not guarantee error recovery.\n- **Adaptation Layer:** Breaks packets into cells for transmission and reassembles them on the receiving end, addressing functions that are part of OSI Layer 4.\n\nATM's efficiency in handling both point-to-point and multicast traffic, along with its support for various types of services, positions it as an integral part of modern telecommunications and distributed system architectures.",
			"styleAttributes":{},
			"x":-2594,
			"y":-3150,
			"width":1248,
			"height":733,
			"color":"1"
		},
		{
			"id":"c40ed1e97f8719cc",
			"type":"text",
			"text":"The text describes several key concepts and details related to the ATM (Asynchronous Transfer Mode) physical layer and its components. Here's a summary:\n\n### ATM Physical Layer:\n- ATM devices (like an ATM adaptor board in a computer) send a continuous stream of cells, even when there's no data (with empty cells transmitted). This makes ATM synchronous at the physical layer.\n- The physical layer can use **SONET** (Synchronous Optical NETwork), which is common in North America, or **SDH** (Synchronous Digital Hierarchy), a European equivalent. Both are designed for high-speed data transfer over fiber networks.\n\n### SONET Frame Structure:\n- A SONET frame consists of a 9 x 90 array of bytes (810 bytes), of which 36 are overhead and 774 are payload.\n- SONET frames are transmitted every 125 microseconds to match the sampling rate of telephone systems, giving a gross data rate of 51.840 Mbps and a net data rate of 49.536 Mbps.\n- SONET standards support different data rates, such as OC-1 (51.840 Mbps), OC-3 (155.520 Mbps), OC-12 (622.080 Mbps), and OC-48 (2.488 Gbps). These standards allow for high-speed communication, with OC-3 c and OC-12 c being important for ATM.\n\n### ATM Layer:\n- ATM cells consist of 53 bytes: 48 bytes for data and 5 bytes for the header. This size was a compromise between American (64-byte) and European (32-byte) preferences, making it inefficient for both voice (too big) and data (too small).\n- The **cell header** layout includes fields like **GFC** (Generic Flow Control), **VPI** (Virtual Path Identifier), **VCI** (Virtual Channel Identifier), **CLP** (Cell Loss Priority), and **CRC** (Cyclic Redundancy Check).\n- Different cell header formats exist for user-to-network communication and switch-to-switch communication, which complicates hardware design.\n\n### ATM Adaptation Layer (AAL):\n- At high data rates (like 155 Mbps), cells can arrive every 3 microseconds. Handling this many interrupts (over 300,000/sec) would overwhelm a CPU. Hence, the **ATM Adaptation Layer (AAL)** manages the disassembly and reassembly of packets into cells.\n- Originally, four AAL classes were defined:\n  1. Constant bit rate traffic (for audio/video).\n  2. Variable bit rate traffic (with bounded delay).\n  3. Connection-oriented data traffic.\n  4. Connectionless data traffic.\n  \n  However, classes 3 and 4 were merged into a new class 3/4.\n  \n- **AAL 5** (Simple and Efficient Adaptation Layer, SEAL) was later introduced for computer-to-computer traffic, allowing for packet transmission with reduced overhead. It uses one bit in the ATM header to mark the last cell of a packet.\n\nThis overview explains the ATM's physical and adaptation layers, focusing on the challenges and solutions for handling data traffic efficiently over high-speed networks.\n\n### 2.2.5. ATM Switching\n\nATM (Asynchronous Transfer Mode) networks are built using copper or optical cables and switches. Figure 2-6 (a) shows an example of an ATM network comprising four switches. Each of these switches connects multiple computers and enables communication between them by routing small data packets, called \"cells,\" through the network. Each switch has four ports, used for both input and output, allowing cells from any computer to reach other computers by traversing one or more switches.\n\nInside a typical switch, as depicted in Figure 2-6 (b), there are input and output lines connected by a parallel switching fabric. Since cells must be switched very quickly—within 3 microseconds at OC-3 speeds—the switch needs to support parallel operations for efficient routing. When a cell arrives at a switch, the switch examines the cell’s VPI (Virtual Path Identifier) and VCI (Virtual Channel Identifier) fields, which help determine the output port based on the pre-established virtual circuit.\n\nOne common issue in ATM switching is when two cells arrive simultaneously on different input lines, both destined for the same output port. The standard allows switches to drop cells, but frequent drops (more than 1 in 10^7 cells) would negatively affect the switch’s performance. A more practical approach is to forward one cell and hold the other in a queue for future transmission. However, if multiple cells are queued for the same output port, this can lead to **head-of-line blocking**, where other cells waiting for free output ports get delayed.\n\nAn alternative switch design alleviates this issue by moving cells to output buffers instead of keeping them in the input queue. This approach enhances performance and avoids head-of-line blocking. Other designs include time-division switches using shared memory, buses, or rings, as well as space-division switches, which provide multiple paths between inputs and outputs.\n\n### 2.2.6. Implications of ATM for Distributed Systems\n\nThe introduction of ATM networks with high bandwidths—155 Mbps, 622 Mbps, and potentially 2.5 Gbps—has several implications for distributed systems, primarily due to the increase in available bandwidth. These effects are most noticeable in wide-area distributed systems.\n\nFor instance, sending a 1-Mbit file across the United States (with a round-trip delay of 30 milliseconds) shows how the increased speed of data transmission can lead to inefficiencies. While transmitting this file at 622 Mbps would take only 1.6 milliseconds, the 30-millisecond delay waiting for an acknowledgment would leave the line idle for 95% of the total time. For shorter messages, the effect is even more pronounced. Consequently, new protocols and architectures will be required to deal with the latency in high-speed networks, especially for interactive applications.\n\n**Flow control** is another challenge. If a sender transmits data at 622 Mbps, the receiver may not have enough buffer space to accommodate large files, such as a 10 GB video. In the time it takes for the receiver to send a STOP signal, up to 20 Mbits of data may have already been transmitted, overwhelming the receiver’s buffer and leading to data loss. Implementing traditional sliding window protocols is inefficient in such cases, as it results in significant underutilization of the virtual circuit’s capacity. Possible solutions include adding more buffer space in the switches or agreeing on a data transmission rate beforehand through **rate control** mechanisms.\n\nAdditionally, ATM’s cell-switching properties create latency issues, such as noticeable delays when sequential requests are made between geographically distant systems. For example, an application in New York making 20 sequential requests to a server in California would experience a 600-millisecond delay, which users would find frustrating. This could lead to centralizing computation to avoid such delays in interactive systems.\n\nAnother ATM-specific problem is the dropping of cells by switches during congestion, which could be detrimental to real-time services like music streaming, where regular delivery is crucial. Therefore, while high-speed ATM networks offer significant opportunities, considerable research is still needed to overcome these challenges and fully leverage their potential in distributed systems.",
			"styleAttributes":{},
			"x":-4318,
			"y":-2942,
			"width":1248,
			"height":670,
			"color":"2"
		},
		{
			"id":"4d4c381692554fa3",
			"type":"file",
			"file":"source-images/Pasted image 20240921195159.png",
			"styleAttributes":{},
			"x":-3349,
			"y":-1135,
			"width":951,
			"height":584
		},
		{
			"id":"de89558b81a51276",
			"type":"file",
			"file":"source-images/Pasted image 20240921195220.png",
			"styleAttributes":{},
			"x":-3349,
			"y":-518,
			"width":951,
			"height":568
		},
		{
			"id":"b77a3f9e54b556cf",
			"type":"text",
			"text":"In the context of Remote Procedure Calls (RPC), parameter passing is a crucial aspect that requires careful handling, especially in distributed systems. The process of taking parameters, packing them into a message, and sending them from the client to the server stub is called *parameter marshaling*. While this process may seem straightforward, there are several challenges, especially when dealing with different machine architectures or data types.\n\n### Basic Example of Parameter Marshaling\n\nConsider a simple RPC function, `sum(i, j)`, that takes two integers and returns their sum. The client stub packs the parameters and the name of the function into a message and sends it to the server stub. The server stub unpacks the message, calls the actual function, and returns the result. If the client and server machines are identical, with scalar parameters like integers or Booleans, this process works without issues.\n\n### Challenges with Different Machine Architectures\n\nIn large distributed systems, where multiple machine types are involved, different machines may represent data (such as integers or characters) in varying formats. For example:\n\n- **Character Encoding**: IBM mainframes may use EBCDIC, while personal computers use ASCII. A character sent from one system might be misinterpreted on another if both systems use different encoding formats.\n  \n- **Integer Representation**: Some systems use 1's complement, while others use 2's complement for representing integers. Similarly, floating-point numbers may also be represented differently.\n  \n- **Byte Ordering**: Machines like Intel processors use *little-endian* byte ordering (low-order byte first), while systems like Sun SPARC use *big-endian* byte ordering (high-order byte first). When messages are transferred byte by byte over the network, this can cause misinterpretation of the data. An integer, for example, may appear vastly different depending on the byte ordering.\n\n### Handling Data Representation and Conversion\n\nTo address these issues, a few strategies can be employed:\n\n1. **Canonical Form Representation**: One way is to define a network-wide standard for representing data types. For instance, integers could use 2's complement, characters use ASCII, and floating-point numbers use IEEE 754 format, with all data transferred in little-endian format. This standardization ensures that data is always interpreted consistently, but it can be inefficient since the client and server may need to perform unnecessary conversions when both use the same internal format.\n   \n2. **Native Format with Indication**: Another approach is for the client to send data in its native format, including a field in the message to indicate the format (e.g., little-endian or big-endian). The server stub can check the format and decide whether conversion is necessary. This approach avoids unnecessary conversions when the client and server use the same format but still handles cases where the formats differ.\n\n### Stubs and Automated Generation\n\nStub procedures, responsible for marshaling and unmarshaling data, are often generated automatically by compilers. These stubs are created based on a formal specification of the server procedure, which includes details about the parameters and the encoding rules. The automatic generation of stubs from a formal specification reduces errors and simplifies the development process.\n\n### Handling Pointers in RPC\n\nOne of the most complex challenges in RPC systems is dealing with pointers. A pointer in one machine’s memory is not valid in another machine's address space. Solutions include:\n\n- **Prohibit Pointers**: One option is to simply disallow pointers in RPC calls. However, this is not practical for many applications where pointers are essential.\n  \n- **Copy/Restore Mechanism**: Instead of passing the pointer itself, the client stub can copy the contents of the memory location the pointer references (e.g., an array) into the message and send it to the server. After the server completes the procedure, the results can be copied back to the client. This method replaces call-by-reference with call-by-value and restore.\n\n- **Optimization**: If the stub knows whether the data pointed to is an input or output parameter, unnecessary copies can be avoided. For instance, if the array is input-only, there is no need to copy it back to the client after the server finishes processing.\n\n### Limitations with Arbitrary Data Structures\n\nAlthough arrays and simple structures can be handled with the copy/restore approach, passing more complex data structures like graphs remains problematic. Some systems attempt to pass actual pointers and generate special server-side code to dereference the pointer and retrieve the necessary data from the client.\n\nIn summary, parameter passing in RPC systems involves several challenges related to data representation, byte ordering, and pointer handling. Efficient and accurate communication between client and server stubs often requires a combination of standardization, format indication, and intelligent data handling strategies.",
			"styleAttributes":{},
			"x":-4333,
			"y":140,
			"width":962,
			"height":967,
			"color":"1"
		},
		{
			"id":"a59c956fec02482a",
			"type":"text",
			"text":"Dynamic binding is a method used in distributed systems to flexibly match clients and servers at runtime, avoiding the inflexibility of hardwiring server addresses in client programs. Instead of embedding static network addresses, dynamic binding allows servers to register themselves with a \"binder\" service, which maintains information about available servers and their interfaces. \n\n### Process of Dynamic Binding:\n\n1. **Server Specification**: Servers provide a formal specification that describes their name, version number, and available procedures. Each procedure lists its parameters, along with their types and direction (in, out, or in-out). For example, Fig. 2-22 shows the specification of a stateless server offering file operations like `read`, `write`, `create`, and `delete`.\n\n2. **Stub Generation**: The server's specification is used by a stub generator to create client and server stubs. The client stub handles communication with the server on behalf of the client, while the server stub processes requests from clients.\n\n3. **Server Registration**: When the server starts, it registers its interface with a binder by sending its name, version number, a unique identifier, and a handle (such as an IP or Ethernet address). The binder then maintains this information, allowing clients to locate the server. Servers can also deregister when they are no longer available.\n\n4. **Client-Server Binding**: When a client calls a remote procedure, the client stub contacts the binder to locate a server matching the required interface. The binder provides the server's handle, which the client uses to communicate directly with the server.\n\n### Advantages of Dynamic Binding:\n- **Flexibility**: It allows for easy updates, such as server relocation, replication, or interface changes, without modifying the client code.\n- **Load Balancing**: The binder can distribute client requests across multiple servers that support the same interface.\n- **Fault Tolerance**: Binders can detect server failures and automatically deregister unresponsive servers.\n- **Authentication**: Servers can restrict access to certain users by setting policies with the binder.\n\n### Disadvantages of Dynamic Binding:\n- **Overhead**: The process of registering, importing, and exporting interfaces adds time and complexity, especially for short-lived client processes.\n- **Bottlenecks**: In large systems, the binder itself can become a bottleneck, requiring the use of multiple synchronized binders, further increasing overhead due to synchronization.\n\nThis dynamic approach ensures that clients and servers can evolve independently and flexibly, which is essential in distributed environments where systems frequently change or scale.",
			"styleAttributes":{},
			"x":-4333,
			"y":1168,
			"width":962,
			"height":866,
			"color":"1"
		},
		{
			"id":"c1f51b33628b9e8c",
			"type":"text",
			"text":"### 2.4. Remote Procedure Call\n\nThe **client-server model** provides a familiar framework for structuring distributed systems, but it has a fundamental flaw: the communication is primarily based on input/output (I/O) operations. Since centralized systems are not typically built around I/O, using it as the foundation for distributed computing has raised concerns. Many researchers believe distributed systems should more closely resemble centralized systems, and reliance on I/O contradicts that vision.\n\nThe breakthrough solution to this problem came from a paper by **Birrell and Nelson (1984)**, which introduced the concept of **Remote Procedure Call (RPC)**. The essence of RPC is simple: it allows programs to call procedures located on different machines. Here’s how it works:\n\n- When a process on **machine A** calls a procedure located on **machine B**, the process on machine A is temporarily suspended.\n- Execution of the called procedure happens on machine B.\n- Data, including parameters, can be sent from A to B, and the result can be returned, all without the programmer needing to deal with explicit message passing or I/O.\n\nThis approach makes distributed computing feel more like centralized computing, as the complexities of network communication are hidden from the programmer.\n\nWhile the concept is elegant, it introduces several challenges:\n- The calling and called procedures are running on **different machines** with **different address spaces**, complicating how data (parameters and results) is transferred.\n- **Machine differences** (e.g., architecture, operating systems) further complicate parameter passing, especially if the systems are not identical.\n- **Failure handling** becomes more complex. Machines can crash independently, and each type of failure (caller or callee machine crashing) needs to be managed in different ways.\n\nDespite these challenges, RPC has become a fundamental and widely-used technique in distributed systems, providing a robust solution for enabling cross-machine procedure calls while abstracting away the complexities of network-based communication.",
			"styleAttributes":{},
			"x":-4323,
			"y":-2011,
			"width":930,
			"height":814
		},
		{
			"id":"01a73f2ef9c06cff",
			"type":"text",
			"text":"### 2.4.1. Basic RPC Operation\n\nTo understand how **Remote Procedure Call (RPC)** works, it’s useful to first review the mechanics of a conventional local procedure call on a single machine. Consider this example in C:\n\n```c\ncount = read(fd, buf, nbytes);\n```\n\nHere, `fd` is an integer, `buf` is an array of characters, and `nbytes` is an integer. In a single machine setup:\n1. **Before the call**, the stack contains local variables of the caller (Fig. 2-17 a).\n2. **During the call**, the caller pushes the parameters onto the stack in reverse order (Fig. 2-17 b). This allows easy access for functions like `printf` to locate the first parameter.\n3. **After the call**, the stack returns to its original state (Fig. 2-17 c).\n\nIn **C**, parameters can be passed by **value** or **reference**:\n- **Call-by-value**: A copy of the value (e.g., `fd` or `nbytes`) is pushed to the stack.\n- **Call-by-reference**: Instead of the actual value, a **pointer** (address of a variable) is pushed. In the case of arrays like `buf`, the address is passed to modify the array in the calling procedure.\n\nAnother parameter passing mechanism is **call-by-copy/restore**, where the variable is copied to the stack at the beginning and copied back after the call.\n\nIn a **remote procedure call (RPC)**, the aim is to make calling a remote procedure as similar as possible to a local one, creating **transparency**. When a local procedure like `read` is called, it interfaces with the kernel and the operating system without the programmer being aware. In an RPC scenario, this transparency is extended to calling procedures across different machines.\n\nIn an RPC setup, the **client stub** plays a crucial role:\n- The client procedure calls the **client stub**, which mimics the local `read` function.\n- Instead of communicating directly with the local kernel, the client stub **packs** the parameters into a message and sends it to the remote machine, asking the kernel to send this message over the network (Fig. 2-18).\n- On the server side, a **server stub** unpacks the message, calls the actual server procedure, and returns the result.\n\n### RPC Process Overview:\n\n1. The **client procedure** calls the client stub.\n2. The client stub **builds a message** and traps to the kernel.\n3. The **client's kernel** sends the message to the remote kernel.\n4. The **remote kernel** passes the message to the server stub.\n5. The **server stub** unpacks the parameters and calls the server procedure.\n6. The **server** executes the requested work and returns the result.\n7. The **server stub** packs the result into a message and sends it back to the client.\n8. The **client's kernel** receives the message.\n9. The **client stub** unpacks the result and returns to the client procedure.\n\nThrough this process, the **client** remains unaware that the procedure was executed remotely. To the client, it behaves as if a local call was made, with no exposure to the underlying message passing. All the complexities of network communication are abstracted by the client and server stubs, providing a seamless interaction for both the client and server. \n\n### Summary of Steps in an RPC:\n1. Client calls the client stub.\n2. Client stub builds a message and traps to the kernel.\n3. Client's kernel sends the message to the remote kernel.\n4. Remote kernel forwards the message to the server stub.\n5. Server stub unpacks the message and calls the server procedure.\n6. Server performs the requested operation.\n7. Server stub packs the result and sends it back.\n8. Client's kernel receives the message.\n9. Client stub unpacks the message and returns the result to the client.\n\nThus, **RPC** effectively makes remote interactions as seamless as local procedure calls without either side needing to manage message passing explicitly.",
			"styleAttributes":{},
			"x":-4339,
			"y":-1135,
			"width":962,
			"height":1185
		},
		{
			"id":"717233eb5c61d46e",
			"type":"text",
			"text":"The issues highlighted in the text relate to challenges in remote procedure calls (RPC) in distributed systems, specifically around client-server communication. Here’s a breakdown of the key challenges and possible solutions:\n\n### 1. **Client Cannot Locate the Server**\n   - **Problem**: The client cannot locate a server due to several reasons, such as the server being down or the client using outdated stubs. This can occur when a client binary is compiled with an older version of the server interface, and by the time it is used, the server has evolved, rendering the client-server communication incompatible.\n   - **Solution**:\n     - The traditional method involves returning an error code like `-1` to indicate failure. However, this may not always work since `-1` can be a valid result in some procedures (e.g., adding 7 and -8).\n     - Another solution is raising an exception. In languages like Ada or C, special error-handling mechanisms (such as exception handlers or signal handlers) can be used for such errors. But this introduces non-transparency and is not available in all languages (e.g., Pascal).\n     - More general error-reporting mechanisms could be explored, such as adding specific error codes (like \"Cannot locate server\") or designing RPC protocols with built-in fault-tolerance.\n\n### 2. **Lost Request Messages**\n   - **Problem**: A client sends a request, but the message is lost in transit, leading to the server never receiving the request.\n   - **Solution**:\n     - Implement a timeout mechanism on the client side. If the client does not receive a reply within a certain period, it resends the request. This method works well as the server cannot differentiate between the original request and the retransmission, and will handle the request as if it was the first.\n     - However, if multiple requests are lost, the client may falsely assume the server is down, leading back to the \"Cannot locate server\" problem.\n\n### 3. **Lost Reply Messages**\n   - **Problem**: The server processes the client's request but the reply is lost, leaving the client unaware that the server has completed the operation. Resending the request could lead to repeated actions, which can be harmful, especially in non-idempotent operations (e.g., transferring money).\n   - **Solution**:\n     - For **idempotent** operations (operations that can be safely repeated without changing the result), resending the request works fine.\n     - For **non-idempotent** operations, the following strategies can be used:\n       - Assign a unique **sequence number** to each request. The server keeps track of the most recent sequence number from each client. If it receives a retransmission with the same sequence number, it recognizes the duplicate and does not execute the operation again.\n       - Use a **bit in the message header** to distinguish initial requests from retransmissions, allowing the server to apply different handling mechanisms for each case.\n\nThese solutions aim to ensure reliable communication in RPC, while handling different failure scenarios (e.g., lost requests, lost replies, server location issues).\n\nIn distributed systems, server and client crashes are common failure scenarios that significantly affect the behavior and reliability of Remote Procedure Calls (RPC). Let's break down the server and client crash scenarios and some proposed solutions:\n\n### **Server Crashes**\n\nServer crashes can complicate the RPC process, particularly when it comes to distinguishing between cases where a server crash happens **before** or **after** a request is executed:\n\n1. **Crash Before Execution**:\n   - If the server crashes before executing the client's request (Fig. 2-24 (c)), the client can safely retransmit the request since no action has been performed yet.\n   \n2. **Crash After Execution but Before Reply**:\n   - In this case (Fig. 2-24 (b)), the request is executed, but the server crashes before sending the reply. Retransmitting the request could result in executing it again, leading to duplicate actions (e.g., transferring money twice), which is undesirable for non-idempotent operations.\n   \n   The challenge here is that the client cannot distinguish between these two cases. When the client’s timer expires, it has no way of knowing if the request was executed or not.\n\n### **Philosophies on Handling Server Crashes**\n\nThere are three main approaches to handling server crashes during RPC:\n\n1. **At Least Once Semantics**:\n   - The client keeps retrying until it receives a reply. This ensures that the request is executed at least once but may result in multiple executions, especially for non-idempotent requests.\n\n2. **At Most Once Semantics**:\n   - The client gives up after detecting a server crash and reports failure. This guarantees that the request is executed at most once, meaning it may not be executed at all, but it ensures no duplicates.\n   \n3. **No Guarantees**:\n   - The simplest approach is to make no promises. If the server crashes, the client may or may not receive a reply, and there's no guarantee about whether the request was executed. This approach is easy to implement but unreliable.\n\nUnfortunately, **exactly once semantics** (where the request is guaranteed to be executed only once) is nearly impossible to achieve in practice due to the inherent uncertainty around when and where a server crash happens.\n\n### **Client Crashes**\n\nClient crashes can leave **orphaned computations** running on the server. These orphans consume resources and can cause confusion if their replies are sent after the client has already rebooted and reissued the same request. There are several proposed strategies to deal with orphans:\n\n1. **Extermination**:\n   - Clients log RPCs before sending them. After rebooting, the log is checked, and any remaining orphans are killed. This is resource-intensive and may not always work, especially in cases of complex RPC chains (e.g., an orphan creates other RPCs).\n\n2. **Reincarnation**:\n   - Time is divided into numbered epochs. After rebooting, the client broadcasts a message starting a new epoch, killing all existing remote computations. This method works well but may leave orphans if the network is partitioned.\n\n3. **Gentle Reincarnation**:\n   - Instead of killing all computations, the system tries to locate the owner of the remote computation. Only when the owner cannot be found is the orphan killed.\n\n4. **Expiration**:\n   - Each RPC has a time limit (T). If it’s not completed in time, it must ask for more time. After the client crashes, the server waits for the time limit to expire before terminating the orphan. This solution requires setting an appropriate T for various tasks, which can be challenging.\n\n### **Practical Considerations**\nNone of the solutions are perfect, and they each come with trade-offs related to complexity, performance, and reliability. Managing these failure cases effectively is crucial for the robustness of distributed systems, but it's clear that handling both server and client crashes involves a delicate balance between ensuring consistency and minimizing resource waste.\n\n",
			"styleAttributes":{},
			"x":-4333,
			"y":2160,
			"width":1448,
			"height":793
		},
		{
			"id":"6161da4681055113",
			"type":"text",
			"text":"The discussion on **Remote Procedure Call (RPC) protocols** highlights several crucial aspects regarding their implementation and performance considerations. Let's break this down:\n\n### 1. **Connection-Oriented vs. Connectionless Protocols:**\n   - **Connection-Oriented Protocols:** These protocols establish a dedicated communication path between the client and server before transferring data. They simplify communication by abstracting away issues like packet loss and acknowledgments, making them ideal for wide-area networks (WANs) where packet loss is more frequent.\n     - **Pros:** Easier communication management, no worries about packet loss or acknowledgments.\n     - **Cons:** Additional overhead can degrade performance, especially in Local Area Networks (LANs), where packet loss is infrequent.\n\n   - **Connectionless Protocols:** These protocols, often used in LAN-based distributed systems, transmit data without pre-establishing a connection. The lack of connection management leads to better performance, especially on reliable LANs.\n     - **Pros:** Better performance due to lower overhead.\n     - **Cons:** Packet loss must be managed manually, but LANs are generally reliable enough for this not to be a significant issue.\n\n### 2. **General-Purpose vs. Custom RPC Protocols:**\n   - **General-Purpose Protocols (e.g., IP/UDP):** Using established protocols like IP or UDP simplifies the design process since these protocols are widely supported, have many existing implementations, and integrate easily with UNIX systems and networks like the Internet.\n     - **Pros:** Saves development time, integrates with existing systems, widely supported.\n     - **Cons:** Performance overhead, as these protocols carry unnecessary fields (e.g., IP headers) designed for broader use cases like internetworking. This can cause inefficiency, especially in LAN-based distributed systems.\n\n   - **Custom RPC Protocols:** These are designed specifically for RPC communication, which eliminates unnecessary features and focuses on performance optimization.\n     - **Pros:** Better performance, no need to handle extraneous cases like packet fragmentation or network bouncing.\n     - **Cons:** Requires designing, testing, and integrating a new protocol, which can be time-consuming and may not gain widespread adoption.\n\n### 3. **Packet and Message Length:**\n   - Efficient communication in RPCs requires transmitting as much data as possible in a single RPC call to minimize overhead. For example, reading a large file in one RPC call is more efficient than splitting it into multiple smaller calls.\n   - **Challenges:** Some RPC systems and networks have limits on packet size (e.g., 8 K for Sun Microsystems’ RPC, or Ethernet’s 1536-byte limit). Splitting RPCs into smaller packets introduces overhead.\n\n### 4. **Acknowledgments (ACKs) and Protocols:**\n   - When large RPCs are split into multiple smaller packets, handling acknowledgments becomes a crucial issue. Two main approaches exist:\n     - **Stop-and-Wait Protocol:** The client sends a packet and waits for an acknowledgment before sending the next one. This ensures no packets are lost but introduces delay, especially if the connection is reliable (as in LANs).\n     - **Blast Protocol:** The client sends all packets at once, and the server acknowledges the entire message after all packets are received. This is more efficient, but it can lead to issues like **receiver overrun**, where the server cannot process incoming packets fast enough.\n\n   - **Selective Repeat:** This more complex protocol handles errors in the blast protocol by selectively requesting the retransmission of only lost packets. It’s more efficient but adds complexity.\n\n### 5. **Flow Control and Overrun Errors:**\n   - **Flow control** ensures that the sender does not overwhelm the receiver with packets it cannot handle due to limited buffer capacity. Overrun errors, caused by receiving too many packets in quick succession, can occur with the blast protocol. To prevent this, techniques like inserting delays between packets or sending a limited number of packets before waiting for an acknowledgment can be employed.\n   - Custom RPC protocols can handle flow control more efficiently by tuning packet transmission to the hardware's timing properties.\n\n### 6. **Handling Lost Acknowledgments:**\n   - One of the trickiest parts of designing RPC protocols is managing the loss of acknowledgments. In some cases, the server might not receive the acknowledgment of a reply it sent. Instead of complicating the protocol by acknowledging the acknowledgments, a timer can be used to discard replies if an acknowledgment is not received within a certain time.\n\n### Conclusion:\nDesigning efficient RPC protocols involves making trade-offs between ease of implementation, performance, and reliability. Connectionless protocols and custom RPC protocols often outperform general-purpose, connection-oriented protocols in LAN environments. However, managing packet size, flow control, and acknowledgments remains a critical challenge, especially when high performance is a priority.",
			"styleAttributes":{},
			"x":-4333,
			"y":3007,
			"width":1448,
			"height":959
		},
		{
			"id":"c2b72bd30ee07613",
			"type":"text",
			"text":"This text provides an in-depth examination of the *critical path* in Remote Procedure Call (RPC) systems, focusing on performance bottlenecks and optimization opportunities. The critical path represents the sequence of steps from when a client makes an RPC call to when the server responds.\n\n### Critical Path Breakdown:\n\n1. **Client Side**:\n    - **Client Stub**: Prepares a message buffer, marshals the parameters (converts them to the proper format), and fills in the message header. \n    - **Trap to Kernel**: The client issues a trap to switch from user mode to kernel mode, where the kernel prepares for message transmission by copying data and addressing it for the network.\n    - **Kernel Operations**: After copying the message to the network interface, the kernel initiates the transmission.\n\n2. **Server Side**:\n    - **Message Reception**: The server's hardware receives the message, generating an interrupt.\n    - **Stub Processing**: The server stub unmarshals the parameters and calls the appropriate server function to perform the requested operation.\n    - **Reply Path**: The server then follows a similar reverse path to send the response back to the client.\n\n### Performance Bottlenecks and Findings:\n\n- The main time-consuming activities include context switching, handling interrupts, and copying messages to/from network interfaces.\n- For larger payloads (e.g., 1440-byte messages), Ethernet transmission dominates the time, while for small RPC calls (null RPC), context switching and packet handling consume most of the time.\n- Schroeder and Burrows' analysis of the DEC Firefly multiprocessor revealed that substantial parallelism in RPC can reduce processing time, but several inefficiencies were noted, such as using UDP and its associated checksum overhead.\n\n### Design Insights:\n- The authors suggest using custom RPC protocols over UDP to avoid unnecessary overheads.\n- Eliminating context switches and copying between user and kernel spaces by using shared address space (as the VAX architecture allowed) can save significant time.\n- Lastly, careful hand-optimization in assembly language resulted in a more uniform distribution of time across different steps, improving performance.\n\nThis detailed analysis provides crucial insights into optimizing RPC systems by focusing on the parts of the critical path that contribute the most to delays.",
			"styleAttributes":{},
			"x":-4333,
			"y":3980,
			"width":1448,
			"height":826
		},
		{
			"id":"eefb3aded58935e2",
			"type":"text",
			"text":"Group communication extends beyond the typical client-server model of remote procedure calls (RPC), where communication is only between two parties. In many distributed systems, there may be a need for one-to-many communication, where a message is sent from a single sender to multiple receivers. This section introduces **group communication**, a mechanism that allows a process to send a message to a group of processes all at once.\n\n### 1. **Group Concept**\nA **group** is a collection of processes that act together in a specified way. The key feature of groups is that when a message is sent to the group, all members receive it. This is a form of **one-to-many communication**, unlike traditional **point-to-point** communication where a message is sent from one sender to one receiver (as illustrated in Fig. 2-30). Processes can join or leave groups dynamically, and a single process can belong to multiple groups at once.\n\nGroup communication can be compared to a social organization where individuals receive announcements from the various groups they belong to. In a computer system, this allows processes to send a message to a group of servers, abstracting away the details about how many servers there are or their locations.\n\n### 2. **Implementing Group Communication**\nThe way group communication is implemented depends on the underlying network and hardware capabilities:\n\n- **Multicasting**: On some networks, a special network address can be assigned to a group. Machines listen to this address, and when a message is sent to it, the message is automatically delivered to all listening machines. Multicasting is efficient because it uses a single packet to reach multiple receivers.\n  \n- **Broadcasting**: If multicasting is unavailable, broadcasting can be used, where a message is sent to all machines on the network. Each machine must check if the packet is intended for it, and discard it if not. While broadcasting is less efficient than multicasting, it still requires only one packet for all group members.\n\n- **Unicasting**: If neither multicasting nor broadcasting is available, the sender can still communicate with the group by sending individual packets to each group member. This method, called **unicasting**, is less efficient because it requires multiple packets (one for each receiver). However, it is still feasible for small groups.\n\n### 3. **Use Cases and Practical Examples**\nGroup communication is particularly useful in systems requiring fault tolerance, where multiple servers work together. For instance, in a distributed file service, a client may send a message to a group of servers to ensure the request is completed, even if one server crashes. Another example is USENET newsgroups, where a message sent to a group is distributed to all members of the group.\n\nIn summary, group communication allows for efficient one-to-many message transmission in distributed systems. The implementation depends on the network, with multicasting being the most efficient option, followed by broadcasting and unicasting. This mechanism abstracts away the complexities of managing multiple receivers, enabling scalable and fault-tolerant distributed services\n\n### 2.5.2. Design Issues in Group Communication\n\nWhen designing group communication systems, several key decisions need to be made, as sending to a group differs significantly from sending to a single process. Below are some important design considerations:\n\n#### 1. **Closed Groups vs. Open Groups**\n   - **Closed Groups**: Only members of the group can send messages to the group as a whole, although outsiders may still communicate with individual members. Closed groups are often used in systems where processes collaborate for a specific task, such as parallel processing in a chess game, where only the group members communicate.\n   - **Open Groups**: Any process, even if it is not a member of the group, can send messages to the group. This is useful in systems involving replicated servers where external clients (not part of the group) need to communicate with the group.\n\n#### 2. **Peer Groups vs. Hierarchical Groups**\n   - **Peer Groups**: In these groups, all processes are equal and decisions are made collectively. There is no central coordinator, making the system resilient to the failure of any single process. However, decision-making may be slower due to the need for collective voting.\n   - **Hierarchical Groups**: These groups have a central coordinator responsible for assigning tasks to the other members (workers). The hierarchy speeds up decision-making but introduces a single point of failure—if the coordinator crashes, the entire group may halt.\n\n#### 3. **Group Membership**\n   Managing group membership is crucial for group communication:\n   - **Centralized Group Server**: A single server manages group membership, handling requests for joining and leaving the group. This method is efficient but suffers from the risk of a single point of failure.\n   - **Distributed Membership**: Membership management can also be handled in a distributed manner, with processes announcing their entry or exit to the group. Crashes pose challenges since there is no voluntary announcement when a process crashes, and the group members must detect this failure experimentally.\n\n   Synchronization of joining and leaving is another critical challenge. New members must immediately start receiving messages after joining, and departed members must stop sending or receiving messages once they leave.\nThe concept of **group addressing** in distributed systems refers to the ability to send a message to a specific group of processes rather than an individual process. There are several ways to achieve this:\n\n1. **Multicast Addressing**: The group is assigned a unique address, and if the network supports multicast, messages sent to this group address are multicast. Each machine that needs to receive the message will get it.\n\n2. **Broadcast Addressing**: If multicast is not supported but broadcast is, the message is broadcast to all machines. Each machine then checks if any of its processes are part of the group. If none are, the message is discarded.\n\n3. **Unicast Addressing**: If neither multicast nor broadcast is available, the sending machine's kernel will maintain a list of machines that have processes belonging to the group and send the message individually to each one (point-to-point).\n\nIn all these cases, the sending process is not concerned with how the message is delivered; it only sends to a group address, and the operating system handles the rest.\n\n### Alternative Group Addressing Methods:\n- **Explicit List of Destinations**: The sender specifies all destination addresses manually. However, this approach has significant drawbacks as it forces user processes to manage group membership and update the list when the group changes. It lacks transparency compared to having the system handle it automatically.\n\n- **Predicate Addressing**: A more dynamic and flexible approach where each message includes a Boolean predicate. The message is sent to all group members, but only those that satisfy the predicate will accept the message. For instance, a message could be targeted to machines with a minimum of 4 MB of free memory.\n\n### Communication Primitives:\nFor group communication, ideally, a unified set of primitives for point-to-point and group messaging is used. If the system relies on **Remote Procedure Call (RPC)**, merging is challenging due to the nature of RPC, which expects one request and one reply. Group communication, by contrast, may involve multiple replies. Hence, group communication often involves explicit send and receive primitives, independent of RPC.\n\n- **Send**: In merged systems, the destination could be a single process or a group. Group messages can be handled in the same way as point-to-point ones.\n  \n- **Receive**: The process waits to receive a message, which could be from either a point-to-point or group communication.\n\nIn some systems, separate primitives like `group_send` and `group_receive` may be introduced to distinguish group messaging from point-to-point messaging.\n\n### Atomicity in Group Communication:\nAtomicity ensures that a message is either delivered to all members of the group or to none, preventing inconsistencies. For instance, in a distributed database, if a message to create a record is missed by some members, future updates could lead to database inconsistencies. Implementing atomic broadcast guarantees that either all members receive the message or none do, simplifying distributed system programming.\n\n### Atomic Broadcast Algorithm:\nTo ensure atomic broadcast even in the presence of machine failures:\n1. The sender sends the message to all group members.\n2. Timers and retransmissions are used to ensure delivery.\n3. When a group member receives the message, it forwards it to all other members, except if it has already received the message.\n4. This ensures that even with machine crashes or packet loss, all surviving members will eventually receive the message.\n\nThis algorithm, though simple, shows that atomic broadcast is possible and lays the foundation for more efficient fault-tolerant methods in distributed systems.",
			"styleAttributes":{},
			"x":-4333,
			"y":4840,
			"width":1448,
			"height":968
		},
		{
			"id":"b9a1eaa089af9b2f",
			"type":"text",
			"text":"In the client-server model described, several important aspects and trade-offs related to distributed systems, networking, and protocol overhead are discussed. Here's a breakdown of the key points:\n\n### 1. **Overhead in Layered Protocols (OSI/TCP/IP)**\n   - **Layered Protocol Overhead**: In a traditional OSI or TCP/IP protocol stack, messages pass through multiple layers, each adding or removing a header. This generates overhead, which impacts performance, particularly in systems with high-speed networks like LANs.\n   - **WANs vs. LANs**: For **wide-area networks (WANs)** with relatively low bandwidth (e.g., 64 Kbps), the overhead caused by the protocol stack is not a major issue because the line capacity itself is the limiting factor. However, for **local area networks (LANs)**, which typically have much higher bandwidth, this overhead becomes more significant, leading to a substantial loss in effective throughput.\n\n### 2. **Client-Server Model Overview**\n   - **Structure**: In the client-server model, the system is organized as **servers** that provide services, and **clients** that request these services. The client-server communication is often simplified to increase efficiency by using **connectionless protocols**.\n   - **Kernel Involvement**: The actual message passing is handled by the kernels of the client and server machines, but in simplified diagrams, this detail may be omitted for clarity.\n   \n### 3. **Advantages of the Client-Server Model**\n   - **Simplicity**: The client-server model typically operates on a simple **request/reply protocol**. The client sends a request, the server performs the task, and then returns the requested data or an error code. No connection setup or teardown is required, which reduces complexity.\n   - **Efficiency**: This simplicity leads to increased **efficiency**. Fewer protocol layers are needed compared to OSI or TCP/IP stacks. In a typical client-server setup on a LAN:\n     - The **physical** and **data link layers** (handled by Ethernet or token ring chips) handle packet delivery.\n     - The **request/reply protocol** operates at layer 5, defining legal requests and replies.\n   - **No Session Management**: Since this model is connectionless, there’s no need for session management, further reducing overhead.\n\n### 4. **System Calls for Message Passing**\n   - The communication services provided by the microkernel are typically minimal, often reduced to two **system calls**:\n     1. **Send**: Sends a message to a destination and blocks the caller until the message is sent.\n     2. **Receive**: Blocks the caller until a message arrives, copies the message into a buffer, and then unblocks the caller.\n\n### 5. **Trade-offs and Use Cases**\n   - **WANs**: OSI or TCP/IP protocols work reasonably well in wide-area distributed systems, where the bandwidth is low, and the protocol overhead is less significant.\n   - **LANs**: For LAN-based systems, where high-speed communication is required, the client-server model with a simple, connectionless protocol is often preferred. This minimizes the overhead and maximizes the effective throughput of the network.\n\n### Summary:\nThe client-server model simplifies communication in distributed systems, especially over LANs, by reducing protocol layers and using a connectionless request/reply model. This improves efficiency compared to more complex, layered protocols, but at the cost of potentially losing some features like session management and guaranteed delivery that are provided by more robust protocol stacks like OSI or TCP/IP.",
			"styleAttributes":{},
			"x":3773,
			"y":5760,
			"width":1476,
			"height":942,
			"color":"1"
		},
		{
			"id":"ceae603f96d71e6f",
			"type":"text",
			"text":"In message-passing systems, blocking and nonblocking primitives serve different needs based on how processes communicate. Here’s a breakdown of the two types:\n\n### Blocking Primitives\n- **Definition**: In blocking (or synchronous) primitives, the sending process is suspended until the message is completely sent. The following instruction is not executed until the message transmission is finished.\n- **Behavior**:\n  - **Send**: The process is blocked while the message is being sent.\n  - **Receive**: The process remains blocked until a message arrives and is placed in the specified buffer.\n- **Advantages**: \n  - Simplicity in implementation and understanding.\n  - No risk of overwriting the message buffer, as the sender cannot proceed until the message is sent.\n- **Disadvantages**: \n  - CPU remains idle during message transmission, which can lead to inefficient use of resources.\n\n### Nonblocking Primitives\n- **Definition**: Nonblocking (or asynchronous) primitives return control to the caller immediately, allowing the sending process to continue executing other instructions while the message is being transmitted.\n- **Behavior**:\n  - **Send**: The process can proceed without waiting for the message to be fully sent.\n  - **Receive**: A nonblocking receive informs the kernel where to store the incoming message but returns control quickly, regardless of whether the message has arrived.\n- **Advantages**: \n  - Improved resource utilization as the CPU can continue processing while waiting for message transmission.\n- **Disadvantages**: \n  - The sender cannot modify the message buffer until the transmission is complete, leading to potential issues if the buffer is reused prematurely.\n  - Complexity in managing when it is safe to reuse the buffer. \n\n### Implementation Strategies for Nonblocking Sends\n1. **Kernel Copy**: The kernel copies the message to an internal buffer, allowing the sender to reuse its buffer immediately. This introduces overhead due to the extra copy.\n2. **Interrupt-Based Notification**: The sender is interrupted once the message has been sent, allowing for direct access to the buffer. However, this can lead to complex programming challenges, including race conditions.\n\n### Blocking vs. Nonblocking Receives\n- Similar distinctions exist for receive operations. Nonblocking receives return control quickly, allowing for polling or explicit wait conditions for message arrival.\n\n### Timeout Handling\n- In blocking systems, if a sender waits indefinitely for a reply, timeouts can be implemented. The sender can specify a maximum wait time, and if no reply is received within that interval, an error status is returned.\n\n### Conclusion\nChoosing between blocking and nonblocking primitives often depends on the specific requirements of the application, such as the need for parallel processing versus the simplicity of implementation. While blocking calls are easier to manage and understand, nonblocking calls offer greater flexibility and resource efficiency, albeit at the cost of increased complexity in handling message buffers and synchronization.\n\n![[Pasted image 20240923003605.png]]\n![[Pasted image 20240923003728.png]]",
			"styleAttributes":{},
			"x":3773,
			"y":6760,
			"width":1476,
			"height":834
		},
		{
			"id":"08c42067dfb6bd81",
			"type":"text",
			"text":"Let’s break down the concepts of buffered and unbuffered primitives in message-passing systems.\n\n### Unbuffered Primitives\n\n**Definition**: In unbuffered message passing, the communication relies on direct synchronization between the sender and receiver. When a process sends a message, it must wait until the receiver is ready to accept it.\n\n#### How It Works:\n1. **Direct Send/Receive**: The sender calls a send operation and directly specifies the receiver’s address. The receiving process must have called receive to be ready to accept the message.\n2. **Message Handling**: If the receiver is not ready (hasn't called receive), the sender's message gets lost, or it may need to retry sending. This can lead to inefficiencies, as the sender may wait or repeatedly attempt to send the message without success.\n\n#### Challenges:\n- **Race Conditions**: If the sender sends a message before the receiver is ready, the message may be discarded, leading to communication failures.\n- **Retry Logic**: The sender may have to implement retry logic, which complicates the communication process.\n\n### Buffered Primitives\n\n**Definition**: Buffered message passing uses mailboxes (buffers) to store incoming messages. This allows senders and receivers to operate more independently, reducing the chances of lost messages.\n\n#### How It Works:\n1. **Mailboxes**: When a process wants to receive messages, it creates a mailbox. Incoming messages addressed to that mailbox are stored there until the process retrieves them.\n2. **Message Handling**: When a message arrives, it is placed in the mailbox. The receiving process can then call receive to take messages out of the mailbox. If no messages are available, it can block or continue working based on the chosen primitive.\n\n#### Benefits:\n- **Decoupling**: Senders and receivers don’t need to be synchronized as closely. A sender can send a message even if the receiver is busy, as long as there is space in the mailbox.\n- **Reduced Message Loss**: Messages are less likely to be lost since they are stored until the receiving process is ready.\n\n#### Challenges:\n- **Limited Capacity**: Mailboxes are finite; if they fill up, additional incoming messages must be handled (either discarded or queued). This can reintroduce similar issues to unbuffered communication if not managed properly.\n- **Management Overhead**: The system must manage buffer allocation and deallocation, which can introduce complexity.\n\n### Summary of Differences\n\n| Feature                  | Unbuffered Primitives                           | Buffered Primitives                              |\n|--------------------------|------------------------------------------------|-------------------------------------------------|\n| **Synchronization**      | Directly synchronized (sender waits for receiver) | Decoupled; sender can send messages independently |\n| **Message Loss**         | Higher chance of message loss if receiver isn’t ready | Lower chance due to message storage in mailboxes |\n| **Complexity**           | Simpler but prone to race conditions           | More complex due to buffer management            |\n| **Capacity**             | No storage, messages are lost if not received   | Limited storage; messages can be queued or discarded |\n\n### Conclusion\nChoosing between buffered and unbuffered primitives depends on the requirements of the system. Buffered primitives allow for more flexible communication at the cost of added complexity, while unbuffered primitives are simpler but can lead to higher risks of message loss. Understanding these concepts is key for designing effective communication in distributed systems. If you have more specific questions or need further clarification on any aspect, just let me know!",
			"styleAttributes":{},
			"x":3773,
			"y":7620,
			"width":1501,
			"height":1034
		},
		{
			"id":"b577ee5f6239613e",
			"type":"text",
			"text":"Let’s explore the concepts of reliable versus unreliable primitives in message passing, especially in the context of client-server communication.\n\n### Unreliable Primitives\n\n**Definition**: Unreliable primitives do not guarantee that messages sent from the client to the server will be received. This means that when a client sends a message, there’s no assurance it has arrived at the server.\n\n#### Characteristics:\n- **No Guarantees**: The system might lose messages due to network issues or other failures.\n- **Implementation Responsibility**: It is up to the application developers to implement any reliability mechanisms if needed, similar to how traditional postal services operate.\n\n### Reliable Primitives\n\n**Definition**: Reliable primitives provide assurances that messages sent will be received by the intended recipient. This typically involves acknowledgment mechanisms.\n\n#### Approaches to Reliability:\n\n1. **Simple Acknowledgment**:\n   - The server sends an acknowledgment (ACK) back to the client after receiving a message.\n   - The client waits for this ACK before proceeding. If the ACK isn't received within a certain time, the client can resend the message.\n\n   **Packet Flow**:\n   - Client sends a request (REQ).\n   - Server responds with a reply (REP).\n   - Server sends an ACK to confirm receipt of the request.\n   - Client acknowledges receipt of the reply.\n\n2. **Reply as Acknowledgment**:\n   - The reply from the server acts as an acknowledgment for the request.\n   - This means the client waits for the reply, which implicitly confirms that the request was received.\n\n   **Packet Flow**:\n   - Client sends a request (REQ).\n   - Server processes the request and sends a reply (REP), which serves as the acknowledgment.\n   - If the reply is lost, the client can resend the request, and the server can recognize it as a duplicate.\n\n3. **Hybrid Approach**:\n   - The server starts a timer when it receives a request. If it sends a reply before the timer expires, the reply serves as the acknowledgment. If the timer expires, the server sends a separate ACK.\n   - This reduces message traffic under normal conditions but allows for acknowledgment in more complex scenarios.\n\n### Packet Types in Client-Server Communication\n\nDifferent types of packets facilitate communication between the client and server:\n\n- **REQ**: A request from the client to the server.\n- **REP**: A reply from the server back to the client.\n- **ACK**: An acknowledgment of receipt, which can be for either requests or replies.\n- **AYA (Are You Alive?)**: A probe to check if the server is still operational.\n- **IAA (I Am Alive)**: The server’s response to an AYA probe.\n- **TA (Try Again)**: Indicates that the server cannot currently process the request because the mailbox is full.\n- **AU (Address Unknown)**: Indicates that the requested address does not correspond to any active process.\n\n### Practical Considerations\n\n1. **Message Size**: Messages larger than a certain size must be split into packets, and these packets may arrive out of order or get lost.\n2. **Acknowledgment Strategies**:\n   - Acknowledge each packet individually or only entire messages.\n   - The choice affects network load and complexity of recovery processes.\n\n### Summary\n\nReliable and unreliable primitives in message passing offer different trade-offs in terms of complexity, efficiency, and robustness. Unreliable primitives are simpler but can lead to lost messages, while reliable primitives implement acknowledgment mechanisms to ensure successful communication. Understanding these concepts is crucial for designing effective distributed systems. If you have further questions or need clarification, feel free to ask!",
			"styleAttributes":{},
			"x":3773,
			"y":8700,
			"width":1501,
			"height":959
		},
		{
			"id":"35a1472e065950e0",
			"type":"text",
			"text":"### Final answer:\n\nA **layered protocol model** uses separate headers for each layer to allow for independent operation and easier upgrades and troubleshooting. It provides modularity. Each layer has its specific responsibilities which translates into its specific header info.\n\n### Explanation:\n\nIn a layered protocol stack such as the **OSI model** or the **TCP/IP model**, each layer adds its own header to the data packet for a very specific reason. Each layer in the protocol stack has a different function, and its header contains the information that it needs to do its job. These responsibilities range from error detection to routing and transportation. Instead of having a single larger header, every layer can independently handle the piece of information relevant to its own function.\n\nFor example, the **data link layer** - Layer 2 in the OSI Model - places its' header that contains MAC addresses information of the sender and receiver for local network transportation. While Layer 3 - the network layer, contains IP information for end-to-end, or device-to-device communication.\n\nHaving separate headers allows for the modular design of network systems. If there were one large header, any changes to **control information** would need to change the entire header structure which would be rather inefficient, thus resulting in difficulty of implementation and upgrade. Furthermore, the division of responsibilities ensures that changes in one layer do not impact others, making troubleshooting easier.",
			"styleAttributes":{},
			"x":-5872,
			"y":-6266,
			"width":945,
			"height":647,
			"color":"5"
		},
		{
			"id":"434570a7699da541",
			"type":"text",
			"text":"An **open system** is one that adheres to standard protocols, allowing it to communicate seamlessly with other systems, regardless of their internal designs. Open systems are designed to facilitate interoperability and data exchange using agreed-upon rules for message format and content.\n\nIn contrast, some systems are **not open** due to proprietary technologies, closed architectures, or non-standard protocols. These systems might restrict communication to only certain applications or devices, limiting their ability to interact with a broader range of systems. This lack of openness can lead to compatibility issues and reduce flexibility in integrating new technologies.",
			"styleAttributes":{},
			"x":-5872,
			"y":-5577,
			"width":945,
			"height":231,
			"color":"5"
		},
		{
			"id":"d6ed35ea1a904cc9",
			"type":"text",
			"text":"![[Pasted image 20240922215424.png]]\n![[Pasted image 20240922215540.png]]\n\nLet's approach this problem step-by-step:\n\n1) First, let's recall some key information:\n   - OC-3 rate is 155.52 Mbps\n   - ATM cell payload is 48 bytes\n   - Interrupt takes 1 μsec (microsecond)\n\n2) For 48-byte packets:\n\n   A) Calculate how many cells are transmitted per second:\n      155.52 Mbps = 155,520,000 bits/sec\n      155,520,000 / (48 * 8) = 405,000 cells/sec\n\n   B) Each cell causes an interrupt, so there are 405,000 interrupts/sec\n\n   C) Each interrupt takes 1 μsec, so total time for interrupts per second:\n      405,000 * 1 μsec = 405,000 μsec = 0.405 sec\n\n   D) Fraction of CPU time:\n      0.405 / 1 = 0.405 or 40.5%\n\n3) For 1024-byte packets:\n\n   A) Calculate how many packets are transmitted per second:\n      155,520,000 / (1024 * 8) = 19,003.90625 packets/sec\n      We'll round this to 19,004 packets/sec\n\n   B) Each packet causes an interrupt, so there are 19,004 interrupts/sec\n\n   C) Total time for interrupts per second:\n      19,004 * 1 μsec = 19,004 μsec = 0.019004 sec\n\n   D) Fraction of CPU time:\n      0.019004 / 1 = 0.019004 or about 1.9%\n\nTherefore, the answers are:\n\n- For 48-byte packets: About 40.5% of CPU time is devoted to interrupt handling.\n- For 1024-byte packets: About 1.9% of CPU time is devoted to interrupt handling.\n\nThis demonstrates how using larger packets can significantly reduce the CPU overhead for interrupt handling in this system.",
			"styleAttributes":{},
			"x":-5950,
			"y":-5283,
			"width":1008,
			"height":540,
			"color":"4"
		},
		{
			"id":"38e95d2ddfb96966",
			"type":"text",
			"text":"To determine the probability that a totally garbled ATM header will be accepted as being correct, we need to consider the structure of the ATM cell header.\n\nAn ATM cell has a 53-byte structure, consisting of:\n- A 5-byte header.\n- A 48-byte payload.\n\nThe header contains several fields, which include:\n- Generic Flow Control (GFC)\n- Virtual Path Identifier (VPI)\n- Virtual Channel Identifier (VCI)\n- Cell Loss Priority (CLP)\n- A checksum (CRC)\n\nAssuming the ATM header is structured such that:\n- Each bit in the header can be either 0 or 1,\n- The header is considered valid if it meets certain criteria (e.g., valid VPI/VCI values, appropriate control bits).\n\nTo calculate the probability of acceptance:\n1. **Determine the total number of possible header combinations**: Since the header is 5 bytes (40 bits), the total number of possible combinations is \\(2^{40}\\).\n\n2. **Determine the number of valid combinations**: This is more complex, as it requires knowledge of how many specific values for VPI, VCI, and other fields are valid. For simplicity, let's assume there are \\(N\\) valid configurations of the header.\n\n3. **Calculate the probability**: The probability \\(P\\) that a randomly chosen header is valid is given by:\n   \\[\n   P = \\frac{N}{2^{40}}\n   \\]\n\nIf we assume that the valid headers are very few compared to the total number of combinations, the probability will be extremely low. For an accurate calculation, we would need specific information about the valid ranges for each field in the ATM header.\n\nIf you have any specific values or additional constraints in mind, please provide those for a more precise calculation!",
			"styleAttributes":{},
			"x":-5934,
			"y":-4737,
			"width":992,
			"height":575,
			"color":"5"
		},
		{
			"id":"29532b7684324117",
			"type":"text",
			"text":"In a nonblocking client-server system where the sender can transmit data directly from user space, it’s important to ensure that the sender is notified when the transmission has been completed, allowing it to reuse its buffer. Here are two approaches to achieve this:\n\n### 1. Callback Mechanism\n\n**Description**: Implement a callback function that the sender registers before initiating the send operation. Once the transmission is completed, the kernel can invoke this callback to inform the sender.\n\n**Implementation Steps**:\n- **Register Callback**: When the sender calls the send function, it passes a pointer to a callback function and any necessary context data.\n- **Asynchronous Notification**: After the message is successfully transmitted, the kernel calls the registered callback function in the context of the sender’s process or thread.\n- **Buffer Reuse**: Within the callback, the sender can safely release or reuse the buffer used for the transmission.\n\n**Advantages**:\n- Immediate notification allows the sender to perform other tasks while waiting for the transmission to complete.\n- Simplifies resource management as the callback handles buffer cleanup.\n\n### 2. Event Notification via Event Loop\n\n**Description**: Use an event loop mechanism where the sender can check the status of the transmission using an event notification system, such as file descriptors or event queues.\n\n**Implementation Steps**:\n- **Event Registration**: When the sender initiates the send operation, it registers for an event notification that indicates when the transmission is complete.\n- **Polling or Event Wait**: The sender continues its processing and periodically checks for events (polling) or blocks waiting for an event to occur (using mechanisms like `select`, `poll`, or `epoll`).\n- **Completion Notification**: Once the transmission is complete, the kernel triggers the event. The sender can then check the event status and safely reuse the buffer.\n\n**Advantages**:\n- This approach integrates well with other asynchronous operations in the application, allowing the sender to manage multiple I/O events efficiently.\n- It provides a flexible way to handle multiple sources of input/output, not just message sending.\n\n### Summary\n\nBoth approaches enable the sender to be notified when a message transmission is complete while minimizing overhead. The callback mechanism is more straightforward for single transmissions, while the event notification system is advantageous for handling multiple concurrent I/O operations efficiently. Each method allows the sender to manage its buffers effectively, ensuring optimal resource utilization.",
			"styleAttributes":{},
			"x":-6040,
			"y":-4100,
			"width":1222,
			"height":756,
			"color":"5"
		},
		{
			"id":"0cf414a56bbd54a4",
			"type":"text",
			"text":"Let’s address each question one by one.\n\n### 1. Are Timeouts Safe to Eliminate in a Fault-Tolerant System?\n\n**Argument Against Eliminating Timeouts**: \nEven in a fault-tolerant system with multiple processors, it might not be safe to completely eliminate timeouts. Here’s why:\n\n- **Network Issues**: While processors may be fault-tolerant, network issues (like packet loss or delays) can still occur, leading to scenarios where a client does not receive a response in a timely manner. Timeouts can prevent the client from hanging indefinitely in such cases.\n- **Unexpected Failures**: Even if the probability of a client or server crashing is low, other unexpected failures (e.g., software bugs, resource exhaustion) can still lead to unresponsive behavior. Timeouts serve as a safeguard against these unpredictable failures.\n- **Performance Optimization**: Timeouts can help optimize resource usage by allowing clients to failover or retry instead of waiting indefinitely, which is especially important in high-load systems where resources are constrained.\n\n### 2. Should the Mailbox Creation Primitive Specify Size?\n\n**Argument For Specifying Size**:\n- **Memory Management**: Specifying the size of the mailbox allows the system to allocate the appropriate amount of memory in advance. This can help prevent resource over-allocation and manage memory more efficiently.\n- **Control Over Resource Usage**: It allows developers to control the maximum number of messages that can be queued, preventing overflow and ensuring predictable behavior, especially in systems with limited resources.\n\n**Argument Against Specifying Size**:\n- **Flexibility**: Not requiring a size allows for dynamic allocation and can simplify the programming model. This flexibility can make it easier to handle varying message loads without needing to reallocate memory or deal with buffer overflows.\n- **Implementation Complexity**: Managing dynamic sizing may complicate implementation, requiring more sophisticated memory management strategies that could introduce overhead and increase the chances of bugs.\n\n### 3. Scheme for a Server to Listen to Multiple Addresses\n\n**Proposed Scheme**: Use a **Multiplexing** or **Listener Registry** approach.\n\n**Implementation Steps**:\n1. **Listener Registration**:\n   - When a server starts, it registers itself with a central kernel or communication manager and specifies all the addresses it intends to listen to.\n   - The server can provide a callback or handler for each address that defines how to process incoming messages.\n\n2. **Event Loop**:\n   - The server operates within an event loop that checks for incoming messages on all registered addresses.\n   - When a message arrives, the kernel identifies the address it was sent to and invokes the corresponding handler registered by the server.\n\n3. **Message Dispatching**:\n   - The kernel can maintain a mapping of addresses to their respective handlers. When a message is received, the kernel dispatches it to the correct handler based on the address.\n   - This can be done using data structures like hash tables for efficient lookup.\n\n4. **Concurrency Handling**:\n   - To handle concurrent requests, the server can spawn threads or use asynchronous processing for each address, allowing it to handle multiple messages simultaneously without blocking.\n\n**Advantages**:\n- This scheme allows a single server process to handle multiple services or functionalities under different addresses without needing separate instances for each service.\n- It can improve resource utilization and simplify management by centralizing the handling of related services.\n\nBy implementing this multiplexing scheme, a server can effectively manage multiple addresses, allowing for scalable and flexible service provision.",
			"styleAttributes":{},
			"x":-6040,
			"y":-3300,
			"width":1222,
			"height":1060
		},
		{
			"id":"79bc9b375a858a59",
			"type":"text",
			"text":"Let's analyze how the `incr` procedure works with two different parameter passing mechanisms: **call-by-reference** and **call-by-copy/restore**.\n\n### 1. Call-by-Reference\n\nIn call-by-reference, the procedure receives pointers to the actual variables. When `incr(i, i)` is called with `i` initially set to 0, both parameters refer to the same memory location (where `i` is stored). \n\n- The procedure `incr` increments both parameters. Since both are referencing the same variable, the effect is:\n  - First increment: `i` becomes 1.\n  - Second increment: `i` becomes 2.\n\nThus, **after the call with call-by-reference**, `i` will be **2**.\n\n### 2. Call-by-Copy/Restore\n\nIn call-by-copy/restore, the variables are copied into the stack for the duration of the call. Here's how it works with `incr(i, i)`:\n\n- When `incr` is called, the values of `i` (which is 0) are copied to the stack, resulting in two separate copies, say `x` and `y`, both initialized to 0.\n- The procedure then increments both copies:\n  - First increment: `x` becomes 1.\n  - Second increment: `y` becomes 1.\n\nAfter the procedure finishes, the original variable `i` remains unchanged because the changes were made to the copies. When the original value is restored (i.e., copied back), it will still be 0.\n\nThus, **after the call with copy/restore**, `i` will remain **0**.\n\n### Summary\n\n- **Call-by-Reference**: `i` becomes **2**.\n- **Copy/Restore**: `i` remains **0**.",
			"styleAttributes":{},
			"x":-6040,
			"y":-2181,
			"width":1309,
			"height":892
		},
		{
			"id":"8a424946764ca1bf",
			"type":"text",
			"text":"To answer this question, we need to understand how big endian and little endian formats work, and how data is transmitted and interpreted between these two systems.\n\n1. SPARC (big endian) representation of integer 2:\n   In binary: 00000000 00000000 00000000 00000010\n   In hexadecimal: 00 00 00 02\n\n2. Data transmission:\n   When data is sent over a network, it's typically sent byte by byte, starting from the lowest memory address.\n\n3. 486 (little endian) interpretation:\n   The 486 will receive these bytes in the same order, but interpret them differently due to its little endian format.\n\n4. 486 interpretation of received data:\n   It will read the bytes as: 02 00 00 00\n\n5. Value interpretation on 486:\n   In little endian, this sequence represents:\n   02 + (00 * 256) + (00 * 256^2) + (00 * 256^3) = 2 * 16777216 = 33554432\n\nTherefore, when a SPARC sends the integer 2 to a 486, the 486 will interpret it as the numerical value 33554432.\n\nThis example illustrates why handling endianness is crucial in distributed systems with different architectures, as mentioned in the text. Without proper conversion or a standardized format for data exchange, such misinterpretations can occur, leading to errors in communication between different systems.",
			"styleAttributes":{},
			"x":-6120,
			"y":-1220,
			"width":1417,
			"height":594
		},
		{
			"id":"e1f8cb80f685f282",
			"type":"file",
			"file":"source-images/Pasted image 20240923010413.png",
			"styleAttributes":{},
			"x":-3349,
			"y":-2011,
			"width":1029,
			"height":589
		},
		{
			"id":"cedc395d7e619a82",
			"type":"file",
			"file":"source-images/Pasted image 20240923010437.png",
			"styleAttributes":{},
			"x":-2278,
			"y":-2011,
			"width":1189,
			"height":677
		},
		{
			"id":"e8947c74a35b643d",
			"type":"text",
			"text":"### Addressing in Distributed Systems\n\nIn a client-server model, effective addressing is crucial for communication between clients and servers. Below are some methods and considerations for addressing processes in distributed systems:\n\n#### 1. Hardwired Machine. Address\n\n- **Description**: The server's address is hardcoded into the client code (e.g., using a constant).\n- **Pros**: Simple implementation.\n- **Cons**: Lack of flexibility and transparency. If the server moves to another machine, the client must be recompiled.\n\n#### 2. Process Numbering\n\n- **Description**: Processes are identified by a combination of machine number and process number (e.g., `243.4` for process 4 on machine 243).\n- **Pros**: Clear identification of processes across machines, allowing multiple processes on a single machine without ambiguity.\n- **Cons**: Users must be aware of the server's location, which reduces transparency.\n\n#### 3. Local Identifiers\n\n- **Description**: Processes can use unique local identifiers (local-id) for communication, typically chosen randomly or sequentially.\n- **Pros**: Simplifies process identification without requiring a machine number, allowing processes to choose identifiers freely.\n- **Cons**: Still requires a way to map local-id to a machine, potentially leading to broadcasting or centralized lookups.\n\n#### 4. Broadcasting\n\n- **Description**: When a process wants to send a message to a destination identified by a unique address, it can broadcast a locate packet. All machines on the network will receive this packet and respond if they match the address.\n- **Pros**: Allows processes to be identified without pre-defined locations.\n- **Cons**: Can lead to network congestion and inefficiencies due to broadcast traffic.\n\n#### 5. Name Servers\n\n- **Description**: Instead of hardcoding addresses, processes are referred to by high-level names (e.g., ASCII strings). Clients query a name server to resolve these names to machine addresses.\n- **Pros**: Enhances transparency and flexibility; changes in server locations do not affect client code.\n- **Cons**: Introduces a centralized component, which can become a bottleneck or single point of failure.\n\n#### 6. Distributed Name Resolution\n\n- **Description**: To enhance reliability, name servers can be replicated. This helps avoid a single point of failure.\n- **Cons**: Maintaining consistency among replicated servers can be complex and introduce additional overhead.\n\n#### 7. Special Hardware Support\n\n- **Description**: Utilize network interface hardware that can recognize and route messages based on process addresses instead of just machine addresses.\n- **Pros**: Potentially reduces load on the network by allowing direct processing at the hardware level.\n- **Cons**: Requires specialized hardware, which may increase system complexity and costs.\n\n### Summary\n\nEach addressing method presents trade-offs between flexibility, transparency, scalability, and complexity. The choice of addressing scheme depends on the specific requirements and constraints of the distributed system being designed. Balancing these factors is essential for building efficient and robust client-server interactions in distributed environments.",
			"styleAttributes":{},
			"x":2800,
			"y":5760,
			"width":934,
			"height":1000
		},
		{
			"id":"8e7445c36b3e9ba7",
			"type":"text",
			"text":"# LAYERED PROTOCOL\n\n\nThe concept of **layered protocols** is foundational to communication in distributed systems, where processes across different machines need to exchange messages effectively despite not sharing memory. Message passing between processes A and B requires adherence to agreed-upon rules, or **protocols**, to ensure that the data being transmitted is interpreted correctly by both parties. For instance, the use of different encoding schemes, such as IBM's EBCDIC and ASCII, without prior agreement would lead to communication errors.\n\n### Importance of Layered Protocols\n\nTo manage the complexity of these agreements across different communication levels—ranging from low-level bit transmission to high-level data formatting—the **Open Systems Interconnection (OSI) Reference Model** was developed by the International Standards Organization (ISO). This model divides communication into **seven distinct layers**, each responsible for specific tasks. By breaking down communication into layers, the model allows different layers to handle tasks such as encoding, transmission, data integrity, and formatting independently, simplifying the design and troubleshooting of communication protocols.\n\nEach layer in the OSI model provides a specific service and interacts with the layer directly above and below it, enabling flexibility. For example, when process A on machine 1 wants to communicate with process B on machine 2, the message passes through each layer, with each one adding relevant information (in the form of headers and sometimes trailers) for the corresponding layer on the receiving side. Upon reaching machine 2, the message is processed by each layer in reverse order until it reaches process B.\n\n### Benefits of the Layered Approach\n\nThe **layered protocol approach** offers significant benefits. For example, companies can update specific layers (e.g., changing communication methods or data content) without affecting the rest of the system. In the analogy of communication between Zippy Airlines and its caterer, Mushy Meals, the bosses (passenger service head and sales manager) can change the content of their discussions (e.g., ordering prime rib instead of rubber chicken), while the secretaries handle the transmission of messages, whether by mail or FAX. These layers operate independently, allowing for changes in one layer without impacting the other.\n\n### Connection-Oriented vs. Connectionless Protocols\n\nThe OSI model also distinguishes between two general types of communication protocols:\n- **Connection-oriented protocols**: Require an established connection before data exchange (e.g., telephone communication).\n- **Connectionless protocols**: Allow sending messages without setting up a prior connection (e.g., dropping a letter in the mailbox).\n\nBy organizing communication into **seven layers** and distinguishing between different types of protocols, the OSI model ensures that distributed systems can communicate reliably, even as technology evolves.",
			"styleAttributes":{},
			"x":-2594,
			"y":-5168,
			"width":1248,
			"height":843,
			"color":"1"
		}
	],
	"edges":[
		{
			"id":"604e9cb19ca1ca5e",
			"styleAttributes":{},
			"fromNode":"caa0079a47d0fa42",
			"fromSide":"bottom",
			"toNode":"38983dab07db9e1e",
			"toSide":"top",
			"color":"1"
		},
		{
			"id":"96897465cf0c5a01",
			"styleAttributes":{},
			"fromNode":"38983dab07db9e1e",
			"fromSide":"right",
			"toNode":"0634f8ccaed92e74",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"519a520d175b31a2",
			"styleAttributes":{},
			"fromNode":"0634f8ccaed92e74",
			"fromSide":"bottom",
			"toNode":"b238264452fc35ed",
			"toSide":"top",
			"color":"1"
		},
		{
			"id":"4bfc0685fad52579",
			"styleAttributes":{},
			"fromNode":"b238264452fc35ed",
			"fromSide":"right",
			"toNode":"746c5f7d7a19a622",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"7443f3c7baf08309",
			"styleAttributes":{},
			"fromNode":"b238264452fc35ed",
			"fromSide":"right",
			"toNode":"f9f4403250cdfa71",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"c725d79e896a60ab",
			"styleAttributes":{},
			"fromNode":"746c5f7d7a19a622",
			"fromSide":"right",
			"toNode":"fa6ed361fc210b90",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"580ee6090223246c",
			"styleAttributes":{},
			"fromNode":"fa6ed361fc210b90",
			"fromSide":"bottom",
			"toNode":"a2ab3c0162ac5bf8",
			"toSide":"top",
			"color":"1"
		},
		{
			"id":"44c94453bb4315f3",
			"styleAttributes":{},
			"fromNode":"a2ab3c0162ac5bf8",
			"fromSide":"right",
			"toNode":"0e6c15a8a6b48e59",
			"toSide":"left",
			"fromEnd":"arrow",
			"color":"1"
		},
		{
			"id":"ce938d47084fcb83",
			"styleAttributes":{},
			"fromNode":"0e6c15a8a6b48e59",
			"fromSide":"bottom",
			"toNode":"1349b49bf72982dc",
			"toSide":"top",
			"color":"1"
		},
		{
			"id":"3643d5449b1ae3ce",
			"styleAttributes":{},
			"fromNode":"c1b80adb9c2ecc20",
			"fromSide":"top",
			"toNode":"a2ab3c0162ac5bf8",
			"toSide":"bottom",
			"color":"1"
		},
		{
			"id":"be87bac59de1c107",
			"styleAttributes":{},
			"fromNode":"c1b80adb9c2ecc20",
			"fromSide":"bottom",
			"toNode":"1523dbbe24d5d69f",
			"toSide":"top",
			"color":"1"
		},
		{
			"id":"a1b91b134a4d7fe0",
			"styleAttributes":{},
			"fromNode":"1349b49bf72982dc",
			"fromSide":"bottom",
			"toNode":"1523dbbe24d5d69f",
			"toSide":"top",
			"color":"1"
		},
		{
			"id":"ebe7bd909237a8be",
			"styleAttributes":{},
			"fromNode":"0b8071fa6382638c",
			"fromSide":"right",
			"toNode":"5a5b0ec268aa5176",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"ca4d6063b5152e04",
			"styleAttributes":{},
			"fromNode":"5a5b0ec268aa5176",
			"fromSide":"bottom",
			"toNode":"2cea445fb0c06380",
			"toSide":"top",
			"color":"1"
		},
		{
			"id":"a5c9e326d1464c92",
			"styleAttributes":{},
			"fromNode":"0b8071fa6382638c",
			"fromSide":"bottom",
			"toNode":"abce02916ead6210",
			"toSide":"top",
			"color":"1"
		},
		{
			"id":"01fc64f5ed259cf7",
			"styleAttributes":{},
			"fromNode":"937047e4f09245c8",
			"fromSide":"right",
			"toNode":"00e1821355256944",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"dbdcaad579dae8f2",
			"styleAttributes":{},
			"fromNode":"937047e4f09245c8",
			"fromSide":"right",
			"toNode":"8686ff98cb236073",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"6d3a805f9fdf1ba5",
			"styleAttributes":{},
			"fromNode":"937047e4f09245c8",
			"fromSide":"right",
			"toNode":"eea9a5f4a956d901",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"a5e33e54a9ee9005",
			"styleAttributes":{},
			"fromNode":"937047e4f09245c8",
			"fromSide":"right",
			"toNode":"6fa7b486e0857436",
			"toSide":"left",
			"color":"1"
		},
		{
			"id":"789ce1ef58945940",
			"styleAttributes":{},
			"fromNode":"8e7445c36b3e9ba7",
			"fromSide":"right",
			"toNode":"9096815183808072",
			"toSide":"left"
		},
		{
			"id":"9c55cd2c3d187f4f",
			"styleAttributes":{},
			"fromNode":"8e7445c36b3e9ba7",
			"fromSide":"top",
			"toNode":"e4cad31ea0e659fb",
			"toSide":"bottom"
		},
		{
			"id":"43a00f4d621cd454",
			"styleAttributes":{},
			"fromNode":"e3f958b1644cc021",
			"fromSide":"right",
			"toNode":"9d57e89998b35402",
			"toSide":"left"
		},
		{
			"id":"01abc7f645752d2b",
			"styleAttributes":{},
			"fromNode":"e3f958b1644cc021",
			"fromSide":"left",
			"toNode":"c40ed1e97f8719cc",
			"toSide":"right"
		}
	],
	"metadata":{}
}