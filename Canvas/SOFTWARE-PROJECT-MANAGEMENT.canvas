{
  "nodes": [
    {
      "id": "bc12e588dcb0f8ce",
      "type": "group",
      "styleAttributes": {},
      "x": -1260,
      "y": 5420,
      "width": 3020,
      "height": 1100,
      "color": "5",
      "label": "CPM"
    },
    {
      "id": "8938a57a81723e81",
      "type": "text",
      "text": "### 3.10.2 Activity Networks\n\nAn **activity network** represents the various tasks in a project, their durations, and their dependencies. Two common representations for activity networks are:\n\n- **Activity on Node (AoN)**: Each activity is represented as a node (rectangular or circular), and task durations are shown within the node. Dependencies between tasks are represented by directional edges (arrows).\n  \n- **Activity on Edge (AoE)**: In this method, tasks are represented on the edges, and task durations are shown on these edges. The nodes represent project milestones.\n\nOriginally, the **Activity on Edge (AoE)** representation was used, but **Activity on Node (AoN)** has become more popular as it's easier to understand and modify.\n\n### Estimating Task Durations\n\nManagers can estimate the time durations for tasks in several ways:\n\n1. **Empirical Assignment**: Managers assign task durations based on experience. However, this can cause friction with developers, as they may feel pressured by these assignments.\n  \n2. **Manager's Estimation**: Some managers estimate durations themselves, believing an aggressive schedule motivates developers. However, this often leads to compromised quality and further delays.\n\n3. **Developer's Estimation**: Developers estimate the time required for their assigned tasks. This method often produces more accurate results and reduces schedule pressure.\n\n### Example: Activity Network for MIS Development Project\n\n#### Task Data from Table 3.7:\n| Task Number | Task               | Duration (Days) | Dependent on Tasks |\n|-------------|--------------------|-----------------|--------------------|\n| T 1          | Specification       | 15              | -                  |\n| T 2          | Design database     | 45              | T 1                 |\n| T 3          | Design GUI          | 30              | T 1                 |\n| T 4          | Code database       | 105             | T 2                 |\n| T 5          | Code GUI part       | 45              | T 3                 |\n| T 6          | Integrate and test  | 120             | T 4, T 5             |\n| T 7          | Write user manual   | 60              | T 1                 |\n\nBased on the above information, an activity network can be created, which shows the tasks, their durations, and how they depend on one another.",
      "styleAttributes": {},
      "x": 840,
      "y": 4320,
      "width": 900,
      "height": 940,
      "color": "2"
    },
    {
      "id": "f1409e1a1bb1f7bd",
      "type": "text",
      "text": "### 3.10.3 Critical Path Method (CPM)\n\nThe **Critical Path Method (CPM)** and **PERT** are project management techniques developed in the 1950 s, which remain popular today. Although they are often merged in tools as **CPM/PERT**, they have key differences. In this section, we'll focus on CPM.\n\n#### Key Terms in CPM:\n\n- **Critical Path**: A sequence of dependent tasks that takes the longest time to complete. Delaying any task on the critical path delays the entire project.\n  \n- **Minimum Time (MT)**: The shortest time to complete the project, determined by the longest path from start to finish.\n\n- **Earliest Start (ES)**: The earliest time a task can start, calculated as the maximum of all paths from the start to this task.\n\n- **Latest Start Time (LS)**: The latest time a task can start without delaying the project. It’s calculated by subtracting the duration of the next task from the latest start time of the following task.\n\n- **Earliest Finish Time (EF)**: The earliest a task can finish, calculated as the sum of the task’s ES and duration.\n\n- **Latest Finish (LF)**: The latest a task can finish without delaying the project. It’s the minimum LS of all succeeding tasks.\n\n- **Slack Time (ST)**: The amount of time a task can be delayed without affecting the project's completion. It is calculated as the difference between LS and ES, or equivalently, LF and EF.\n\n#### CPM Calculation Steps:\n1. **Compute ES and EF for each task**:\n   - ES is the largest EF of all predecessor tasks.\n   - EF is the sum of ES and task duration.\n   \n2. **Compute LS and LF for each task**:\n   - LF is the smallest LS of all successor tasks.\n   - LS is LF minus task duration.\n\n3. **Compute Slack Time (ST)**:\n   - ST = LF - EF.\n\n#### Example: MIS Development Project\n\nUsing the activity network of the MIS project (Figure 3.9), the **ES** and **EF** for each task have been calculated (Figure 3.10), followed by the **LS** and **LF** (Figure 3.11). The table below summarizes these project parameters:\n\n| Task               | ES  | EF  | LS  | LF  | ST  |\n|--------------------|-----|-----|-----|-----|-----|\n| Specification       | 0   | 15  | 0   | 15  | 0   |\n| Design database     | 15  | 60  | 15  | 60  | 0   |\n| Design GUI part     | 15  | 45  | 90  | 120 | 75  |\n| Code database       | 60  | 165 | 60  | 165 | 0   |\n| Code GUI part       | 45  | 90  | 120 | 165 | 75  |\n| Integrate and test  | 165 | 285 | 165 | 285 | 0   |\n| Write user manual   | 15  | 75  | 225 | 285 | 210 |\n\n#### Critical Path\n\nThe critical path consists of tasks with zero slack time. These tasks form the longest path in the project and must be completed on time to avoid delays. The critical path in the activity network is shown with a thick arrow in **Figure 3.9**.",
      "styleAttributes": {},
      "x": 840,
      "y": 5440,
      "width": 900,
      "height": 1060,
      "color": "2"
    },
    {
      "id": "9bfc965cc1c7d7c4",
      "type": "file",
      "file": "source-images/Pasted image 20240915023335.png",
      "styleAttributes": {},
      "x": 1840,
      "y": 4440,
      "width": 1160,
      "height": 557
    },
    {
      "id": "3ecffd032061642e",
      "type": "file",
      "file": "source-images/Pasted image 20240915030047.png",
      "styleAttributes": {},
      "x": 40,
      "y": 5440,
      "width": 768,
      "height": 400
    },
    {
      "id": "9c6c9fac76d3474a",
      "type": "file",
      "file": "source-images/Pasted image 20240915030101.png",
      "styleAttributes": {},
      "x": 40,
      "y": 5878,
      "width": 661,
      "height": 622
    },
    {
      "id": "387ac2aa506514d8",
      "type": "text",
      "text": "Scheduling project tasks is a key part of project planning. According to Boehm, there’s a limit to how much a schedule can be shortened, regardless of adding more personnel or equipment.\n\nOnce a project starts, the manager monitors progress and takes corrective action if needed to prevent delays. To schedule activities, the project manager follows these steps:\n\n1. Identify major activities.\n2. Break activities into smaller tasks.\n3. Determine task dependencies.\n4. Estimate time durations.\n5. Create an activity network.\n6. Determine task start and end dates.\n7. Identify the critical path, which defines the project duration.\n8. Allocate resources to tasks.\n\nThe process begins by identifying necessary activities and breaking them down into smaller tasks, which are assigned to developers. The work breakdown structure technique helps with this. Task dependencies, where one task must follow another, are represented as an activity network.\n\nAfter creating the activity network, resources are allocated using a Gantt chart. A PERT chart is then developed to aid in project monitoring and control.",
      "styleAttributes": {},
      "x": -240,
      "y": 4040,
      "width": 880,
      "height": 560,
      "color": "1"
    },
    {
      "id": "4694cc66a0ba5993",
      "type": "file",
      "file": "source-images/Pasted image 20240915030146.png",
      "styleAttributes": {},
      "x": -1220,
      "y": 5480,
      "width": 1211,
      "height": 537
    },
    {
      "id": "7993648dcc6f3f73",
      "type": "text",
      "text": "### Work Breakdown Structure (WBS)\n\nWork Breakdown Structure (WBS) is a technique used to break down project activities into smaller, manageable tasks.\n\n- **Goal**: The main goal of WBS is to determine which tasks should be taken up, when, and by whom.\n- **Tasks**: The smallest units of work are called **tasks**, which are subject to management planning and control.\n\n### Importance of Task Breakdown\n\n- **Reason for Breakdown**: Breaking down project activities into tasks helps determine the time frame for each task and enables tracking of project progress.\n- **Milestones**: Each major activity has a milestone. These milestones are monitored to ensure that the project stays on schedule.\n- **Task Monitoring**: If a milestone is delayed, the project manager tracks tasks more closely to ensure the overall deadline is met.\n\n### WBS Representation\n\n- **Tree Structure**: Activities, sub-activities, and tasks are represented as a tree, with the root node labeled by the project name. Sub-activities are represented as children of the parent activity.\n\n### When to Stop Decomposition\n\n- **Two-week Rule**: Decomposition stops when a task takes about two weeks to complete.\n- **Hidden Complexities**: Decomposition continues until hidden complexities are exposed, allowing tasks to be better understood and assigned.\n- **Opportunities for Reuse**: Decomposition can reveal opportunities to reuse existing components.\n\n### Task Granularity\n\n- **Fine-Grained Tasks**: Smaller tasks allow for more frequent milestones and close project monitoring. However, too much subdivision can lead to excessive chart revisions and project management overhead.\n- **Coarse-Grained Tasks**: Larger tasks may cause delays to go unnoticed until it’s too late to correct them. But if tasks are too small (e.g., 1–2 days), they require frequent updates, making monitoring inefficient.\n  \n### Balance in Task Breakdown\n\n- **Parallelism**: Breaking tasks into smaller units allows for more parallelism, potentially speeding up development.\n- **Optimal Granularity**: Tasks should typically take one to two weeks to execute. Tasks shorter than this create unnecessary overhead in project management.\n\n### Example: Management Information System (MIS) Project Breakdown\n\n#### Main Activities:\n1. **Requirements Specification**\n   - Elicit requirements\n   - Analyze requirements\n   - Document requirements\n\n2. **Design**\n   - System design\n   - Database design\n   - User interface design\n\n3. **Code**\n   - Implement modules\n   - Integrate modules\n   - Code review\n\n4. **Test**\n   - Unit testing\n   - Integration testing\n   - System testing\n\n5. **Document**\n   - User manual\n   - Technical documentation",
      "styleAttributes": {},
      "x": 840,
      "y": 3380,
      "width": 933,
      "height": 750,
      "color": "2"
    },
    {
      "id": "8fcffa9f1ff341a1",
      "type": "file",
      "file": "source-images/Pasted image 20240915023357.png",
      "styleAttributes": {},
      "x": 1840,
      "y": 3420,
      "width": 1040,
      "height": 643
    },
    {
      "id": "2eccf479bb893659",
      "type": "link",
      "url": "https://youtu.be/Us5YtgvfomQ?si=nLyaae-h1CJYguwD",
      "styleAttributes": {},
      "x": 1840,
      "y": 5440,
      "width": 1840,
      "height": 1060
    },
    {
      "id": "2519b72f37b9558e",
      "type": "text",
      "text": "# PERT Charts\n\nThe **Program Evaluation and Review Technique (PERT)** is a more advanced form of activity chart, designed to handle uncertainty in project schedules by incorporating probabilistic task durations. Unlike the **Critical Path Method (CPM)**, which assumes fixed task durations, PERT recognizes that actual durations may vary and treats task completion times as random variables with probability distributions.\n\n#### Key Concepts in PERT:\n1. **Uncertainty in Task Durations**: \n   - Project managers only estimate the time a task might take, and there is often considerable uncertainty. PERT charts account for this by representing the duration of tasks using a probability distribution, typically assuming a normal distribution.\n\n2. **Three Time Estimates**:\n   Each task in a PERT chart is associated with three time estimates:\n   - **Optimistic (O)**: The shortest possible time to complete the task, assuming everything goes better than expected.\n   - **Most Likely (M)**: The most likely time to complete the task under normal conditions.\n   - **Pessimistic (W)**: The longest possible time to complete the task, assuming significant delays.\n\n3. **Mean Estimated Time (ET)**:\n   The mean or expected time for each task is computed using the formula:\n   \\[\n   ET = \\frac{O + 4 M + W}{6}\n   \\]\n   This formula weights the most likely estimate (M) more heavily since it's considered the most probable scenario.\n\n4. **Standard Deviation (ST)**:\n   The standard deviation of the task duration is used to measure the variability or uncertainty in the estimates. It is computed as:\n   \\[\n   ST = \\frac{W - O}{6}\n   \\]\n   This provides an estimate of how spread out the possible completion times could be around the mean.\n\n5. **Normal Distribution Assumption**:\n   - The entire duration range for a task is assumed to lie within ±3 times the standard deviation from the mean, i.e., between \\( M - 3 \\times ST \\) and \\( M + 3 \\times ST \\).\n   \n6. **Critical Path in PERT**:\n   Similar to CPM, PERT identifies a critical path, which is the longest path of dependent tasks that determines the project's minimum duration. However, in PERT, the critical path analysis becomes more complex due to the variability in task durations. Different permutations of task times can result in multiple potential critical paths.\n   \n   The critical path in a PERT chart is represented using thicker arrows to highlight its importance. Since task durations are uncertain, the critical path may change as tasks progress, making it necessary to continually assess the likelihood of meeting project milestones.\n\n#### Example: MIS Problem PERT Chart\nThe PERT chart for the **MIS problem** (referenced in **Figure 3.9**) is shown in **Figure 3.12**. This chart illustrates the statistical variations in task estimates, using the optimistic, most likely, and pessimistic time estimates to calculate the expected time (ET) and standard deviation (ST) for each task.\n\n#### Advantages of PERT:\n- **Probabilistic Milestones**: PERT provides the probability of achieving specific project milestones based on the probabilistic completion times of tasks along the critical path.\n- **Handling Uncertainty**: PERT is useful in projects where there is significant uncertainty about task durations, offering a more flexible and realistic project timeline.\n\n",
      "styleAttributes": {},
      "x": 840,
      "y": 6600,
      "width": 900,
      "height": 1080
    },
    {
      "id": "18e9b6da4880d680",
      "type": "text",
      "text": "# Constructive Cost Estimation Model (COCOMO)\n\nThe Constructive Cost Estimation Model (COCOMO), proposed by Barry Boehm in 1981, provides a structured approach to project estimation. COCOMO employs a three-stage process to estimate project size, effort, and cost. The model uses both single and multivariable estimation techniques across its different stages to refine initial estimates into more accurate predictions.\n\nThe three stages of the COCOMO estimation technique are:\n\n1. **Basic COCOMO**\n2. **Intermediate COCOMO**\n3. **Complete COCOMO**\n\nEach stage serves a specific purpose in the estimation process, progressively refining the estimates based on increasing levels of detail and project understanding.",
      "styleAttributes": {},
      "x": 1500,
      "y": 1720,
      "width": 1140,
      "height": 420,
      "color": "3"
    },
    {
      "id": "b31b2709ba5575fa",
      "type": "text",
      "text": "# Project Estimation Techniques\n\nEstimating various project parameters such as size, effort, duration, and cost is crucial for effective project planning. Accurate estimations help in quoting appropriate project costs, planning resources, and scheduling. There are several estimation techniques, broadly classified into three categories:\n\n#### 3.5.1 Empirical Estimation Techniques\n\n**Empirical estimation techniques** are based on educated guesses and prior experience with similar projects. Although these techniques rely on common sense and subjective decisions, they have been formalized over time. Two notable empirical techniques are:\n\n- **Expert Judgment**: Involves consulting experts with experience in similar projects to estimate parameters.\n- **Delphi Technique**: A structured method where a panel of experts provides estimates anonymously. The estimates are aggregated, and experts revise their estimates based on feedback until a consensus is reached.\n\n#### 3.5.2 Heuristic Techniques\n\n**Heuristic techniques** use mathematical models to estimate project parameters. These models can be categorized into:\n\n- **Single Variable Models**: Estimate parameters based on a single independent variable. For example, the COCOMO model estimates cost based on software size. The model follows a form like:\n  \n  \\[\n  \\text{Estimated Parameter} = c_1 \\times e^{d_1}\n  \\]\n  \n  where \\( e \\) is the independent variable (e.g., software size), and \\( c_1 \\) and \\( d_1 \\) are constants derived from historical data.\n\n- **Multivariable Models**: Estimate parameters based on multiple independent variables. For example, an intermediate COCOMO model considers various factors like size, complexity, and team capability:\n  \n  \\[\n  \\text{Estimated Resource} = c_1 \\times p_1^{d_1} + c_2 \\times p_2^{d_2} + \\ldots\n  \\]\n  \n  where \\( p_1, p_2, \\ldots \\) are independent variables, and \\( c_1, c_2, d_1, d_2, \\ldots \\) are constants. These models generally provide more accurate estimates because they consider multiple influencing factors.\n\n#### 3.5.3 Analytical Estimation Techniques\n\n**Analytical estimation techniques** are based on scientific principles and basic assumptions about the project. These techniques offer a more rigorous approach compared to empirical and heuristic methods. An example is:\n\n- **Halstead’s Software Science**: This technique estimates software maintenance effort by analyzing software complexity based on metrics like the number of operators and operands. It provides a mathematical approach to understanding software metrics and can be more accurate in estimating maintenance efforts compared to other techniques.\n\nBy using these estimation techniques, project managers can better predict project parameters and make informed decisions throughout the project lifecycle.",
      "styleAttributes": {},
      "x": 220,
      "y": 1440,
      "width": 1060,
      "height": 560,
      "color": "2"
    },
    {
      "id": "5532fa8dde242548",
      "type": "text",
      "text": "# Empirical Estimation Techniques\n\nEmpirical estimation techniques, although formalized to some extent over the years, still fundamentally rely on educated guesses. These techniques are relatively straightforward and can provide reasonably accurate estimates. Two prominent empirical estimation techniques are **Expert Judgement** and **Delphi Cost Estimation**.\n\n#### Expert Judgement\n\nExpert judgement is a commonly used method for estimating project size. In this technique, an expert provides an educated guess regarding the size of the project after a thorough analysis. Typically, the expert evaluates the cost of different components, such as modules or subsystems, and combines these estimates to arrive at an overall estimate for the project. \n\nHowever, this technique has several shortcomings. The accuracy of the estimates can be affected by human errors and individual biases. Additionally, an expert might overlook certain factors inadvertently. For instance, an expert might be well-versed in database and user interface components but lack knowledge about computer communication aspects, potentially leading to inaccurate estimates. Consequently, the estimate produced by a single expert may not always be reliable.\n\nTo address some of these issues, a more refined approach involves using a group of experts. This method reduces the likelihood of errors due to individual oversight, lack of familiarity with specific project aspects, and personal biases. It also helps counteract the tendency of an individual expert to provide overly optimistic estimates in an effort to win a contract. Nonetheless, estimates produced by a group of experts can still exhibit biases. For example, the group may be influenced by political or social factors, and decisions may be swayed by more assertive members of the group.\n\n#### Delphi Cost Estimation\n\nThe Delphi cost estimation technique aims to overcome some of the limitations associated with expert judgement. This technique involves a structured process with a group of experts and a coordinator. The coordinator provides each estimator with the Software Requirements Specification (SRS) document and a form for recording their cost estimates. Estimators then complete their estimates anonymously and submit them to the coordinator. They also note any unusual characteristics of the product that influenced their estimates.\n\nThe coordinator compiles the estimates and prepares a summary, including any notable rationales provided by the estimators. This summary is then distributed back to the estimators. Based on this information, the estimators revise their estimates. This iterative process is repeated for several rounds. To prevent undue influence, no discussions among the estimators are allowed during the estimation process. Allowing discussions could lead to estimators being swayed by the rationale of more experienced or assertive members.\n\nAlthough Delphi estimation is more time-consuming and labor-intensive than expert judgement, it mitigates the risk of unjust influence by senior or assertive members, leading to potentially more accurate and balanced estimates.",
      "styleAttributes": {},
      "x": 1500,
      "y": 1120,
      "width": 1140,
      "height": 463,
      "color": "3"
    },
    {
      "id": "e4f4e23185bb37af",
      "type": "text",
      "text": "# Function Point (FP) Metric\n\nThe Function Point (FP) metric, proposed by Albrecht and Gaffney in 1983, addresses many of the shortcomings of the Lines of Code (LOC) metric. It has gained popularity due to its advantages, particularly in its ability to be computed from the problem specification itself, unlike LOC which can only be accurately determined after code development is complete.\n\n#### Conceptual Foundation\n\nThe FP metric is based on the idea that the size of a software product is related to the number of different high-level functions or features it supports. Each feature contributes to the size of the product, as more features generally require more effort to develop. \n\nHowever, the effort to develop different features varies significantly. For example, displaying a help message in banking software is less complex than handling actual banking transactions. To refine the measure, the FP metric also considers the number of input and output data items and the files accessed by each function. The assumption is that more data items and files accessed indicate higher complexity.\n\n#### Function Point (FP) Computation\n\nThe FP metric is computed in three main steps:\n\n1. **Compute Unadjusted Function Points (UFP):**\n   UFP is calculated using a heuristic expression based on the number of inputs, outputs, inquiries, files, and interfaces. The formula is:\n   \n   \\[\n   \\text{UFP} = (\\text{Number of inputs} \\times 4) + (\\text{Number of outputs} \\times 5) + (\\text{Number of inquiries} \\times 4) + (\\text{Number of files} \\times 10) + (\\text{Number of interfaces} \\times 10)\n   \\]\n\n   - **Number of inputs:** Data items entered by the user.\n   - **Number of outputs:** Outputs such as reports, screen displays, or error messages.\n   - **Number of inquiries:** User commands that require system actions without data input.\n   - **Number of files:** Logical files representing related data.\n   - **Number of interfaces:** Mechanisms for data exchange with other systems.\n\n2. **Refine UFP:**\n   UFP is refined by adjusting for the complexity of each parameter (input, output, etc.). Each parameter is categorized as simple, average, or complex, and the weights are adjusted accordingly. For example, simple inputs might be worth 3 FPs, average inputs 4 FPs, and complex inputs 6 FPs.\n\n   | Type | Simple | Average | Complex |\n   |------|--------|---------|---------|\n   | Input | 3      | 4       | 6       |\n   | Output| 4      | 5       | 7       |\n   | Inquiry| 3     | 4       | 6       |\n   | Files | 7      | 10      | 15      |\n   | Interfaces | 5  | 7       | 10      |\n\n3. **Compute FP:**\n   FP is further refined to account for project-specific factors that influence development effort. This involves assessing 14 parameters (such as performance requirements, extent of reuse, etc.), each rated from 0 to 6. The sum of these ratings gives the Degree of Influence (DI). The Technical Complexity Factor (TCF) is calculated as:\n\n   \\[\n   \\text{TCF} = 0.65 + (0.01 \\times \\text{DI})\n   \\]\n\n   FP is then computed as:\n\n   \\[\n   \\text{FP} = \\text{UFP} \\times \\text{TCF}\n   \\]\n\n   The TCF adjusts the UFP to reflect the overall project complexity.\n\n#### Example Problem: Supermarket Software\n\nTo determine the FP measure for a supermarket software, follow these steps:\n\n- **Identify the inputs:** Residence address, telephone number, driving license number.\n- **Identify the outputs:** Customer identity card, purchase value, surprise gift list.\n- **Identify inquiries:** Check purchase value, print gift list.\n- **Identify files:** Customer records.\n- **Identify interfaces:** Not explicitly mentioned in the problem.\n\nGiven the problem's characteristics are average, calculate the UFP using the standard weights, refine based on complexity (if applicable), and then adjust for project-specific factors using the TCF formula.\n\n#### Shortcomings of FP Metric\n\n1. **Algorithmic Complexity Not Accounted For:** FP does not consider the complexity of algorithms used to implement features, assuming all functions require the same effort to develop.\n2. **Subjectivity:** The grouping of data items and determining function complexity can be subjective, leading to potential inconsistencies in FP measurements.\n\n**Feature Point Metric:** An extension called Feature Point Metric addresses these shortcomings by incorporating algorithm complexity, providing a more accurate reflection of development effort.\n\nOverall, while the FP metric provides a more comprehensive measure of software size compared to LOC, it is not without its challenges, particularly regarding subjective assessments and complexity adjustments.",
      "styleAttributes": {},
      "x": 1400,
      "y": 563,
      "width": 1020,
      "height": 447,
      "color": "3"
    },
    {
      "id": "41ba5c3df05a2681",
      "type": "text",
      "text": "# Lines of Code (LOC)\n\n**Lines of Code (LOC)** is a straightforward and widely used metric for estimating project size. It measures the size of a software project by counting the number of lines in the source code, excluding comments and header lines. Despite its simplicity and ease of measurement, LOC has several notable advantages and limitations.\n\n#### **Advantages of LOC**\n\n1. **Simplicity and Direct Measurement:**\n   - LOC is easy to measure and understand. It directly counts the lines of code, making it a straightforward metric to apply. Tools for counting LOC are readily available and can automate this process.\n\n2. **Direct Correlation with Code Size:**\n   - LOC provides a clear measure of the codebase size. It can be helpful for understanding the volume of code that has been written and for estimating the time required for code review and maintenance.\n\n#### **Limitations of LOC**\n\n1. **Focuses Solely on Coding Activity:**\n   - LOC measures only the coding aspect of software development, neglecting other critical activities such as specification, design, and testing. As a result, it may not accurately reflect the total effort required for a project. For example, a project with complex design and testing phases might have a small codebase but require substantial effort.\n\n2. **Varies with Coding Style:**\n   - LOC can vary significantly based on coding style and programming practices. For instance, different programmers may write the same functionality with different code layouts, or use different algorithms and constructs. This variability can lead to different LOC counts for essentially similar functionality.\n\n3. **Poor Correlation with Code Quality and Efficiency:**\n   - A higher LOC count does not necessarily indicate better quality or more efficient code. A large codebase might result from poor coding practices or inefficient algorithms. Conversely, well-written, efficient code might be more concise but achieve the same functionality with fewer lines.\n\n4. **Penalizes Use of Higher-Level Languages and Code Reuse:**\n   - LOC does not account for the use of higher-level programming languages or code reuse. A programmer who uses library routines or higher-level constructs might produce less code, which could be misinterpreted as reduced effort or productivity. This might discourage the use of effective programming practices that lead to more efficient and maintainable code.\n\n5. **Does Not Address Logical and Structural Complexity:**\n   - LOC only measures the lexical complexity of code and does not reflect the logical and structural complexity of the program. Two programs with the same LOC might have vastly different complexities and development efforts. For example, a program with many nested loops and decision constructs is generally more complex than one with simple, sequential logic.\n\n6. **Difficult to Estimate Early in the Project:**\n   - Estimating LOC accurately at the beginning of a project is challenging. Since LOC can only be precisely measured after the code is written, project managers often rely on rough estimates or historical data. This makes LOC less useful for initial project planning and estimation.\n\n#### **Conclusion**\n\nWhile LOC is a useful and easy-to-measure metric, its limitations make it less effective as a sole indicator of project size and complexity. It fails to account for non-coding aspects of development, coding style variations, code quality, and logical complexity. As such, it is often used in conjunction with other metrics, like Function Points (FP), to provide a more comprehensive view of project size and effort.",
      "styleAttributes": {},
      "x": 1400,
      "y": 20,
      "width": 1020,
      "height": 503,
      "color": "3"
    },
    {
      "id": "3b122a63a709e73a",
      "type": "text",
      "text": "# METRICS FOR PROJECT SIZE ESTIMATION\n\n### **Metrics for Project Size Estimation**\n\nAccurate project size estimation is crucial for predicting project effort, completion time, and total cost. The term \"project size\" refers to more than just the amount of source code or the size of the executable. Here, we will discuss the two popular metrics used to measure project size: **Lines of Code (LOC)** and **Function Points (FP)**. Each has its advantages and disadvantages.",
      "styleAttributes": {},
      "x": 400,
      "y": 363,
      "width": 800,
      "height": 280,
      "color": "2"
    },
    {
      "id": "ab425b93022cdc22",
      "type": "text",
      "text": "## Introduction\n\nEffective project management is a critical factor in the success of software development projects. Many projects fail not due to a lack of technical expertise or resources but because of inadequate project management practices. Thus, learning the latest software project management techniques is essential for ensuring project success.\n\n## Complexity of Software Project Management (TICCEU)\n\nSoftware project management is more complex than managing other types of projects. Several factors contribute to this complexity, including:\n\n### Invisibility\n\nSoftware remains invisible during development, making it difficult to assess progress. Unlike physical projects, such as construction, where visual inspection can monitor progress, software projects require tracking milestones and deliverables to gauge progress.\n\n### Changeability\n\nSoftware is easier to change than hardware, leading to frequent requirement changes, especially in later stages. These changes arise from modifications in business practices, underlying software, or hardware.\n\n### Complexity\n\nEven moderate-sized software projects involve numerous interacting components, increasing the project's complexity. Managing these intricate interactions introduces additional risks and challenges.\n\n### Uniqueness\n\nEach software project typically has unique features, unlike more predictable projects in other domains like manufacturing. This uniqueness forces project managers to confront unanticipated issues.\n\n### Exactness of the Solution\n\nSoftware components, like function calls, require precise conformity to their definitions. This exactness makes it challenging to get the software up and running and complicates reusing components.\n\n### Team-Oriented and Intellect-Intensive Work\n\nSoftware development is a team-oriented and intellectually intensive process, involving significant interaction among team members. This further adds to the complexity compared to labor-intensive projects where autonomy is higher.\n\n## Responsibilities of a Software Project Manager\n\nA software project manager's responsibilities are broad and varied, ranging from building team morale to making customer presentations. These responsibilities can be classified into two main categories: project planning and project monitoring and control.\n\n### Project Planning\n\nProject planning involves estimating various characteristics of a project and organizing activities accordingly. This phase begins after the feasibility study and continues through the project's progress, with revisions as more data becomes available.\n\n### Project Monitoring and Control\n\nOnce development begins, project monitoring and control activities ensure the project stays on track. As unforeseen situations arise, the project manager may need to adjust plans to address these challenges.\n\n## Skills Necessary for Managing Software Projects\n\nEffective project management requires more than just theoretical knowledge. Project managers must possess good judgment, decision-making capabilities, communication skills, and the ability to manage teams. While some skills, like team building and managerial presentations, are acquired through experience, a sound understanding of software project management techniques remains crucial.",
      "styleAttributes": {},
      "x": -780,
      "y": -480,
      "width": 940,
      "height": 900,
      "color": "1"
    },
    {
      "id": "2597214111e33079",
      "type": "text",
      "text": "Project planning is a critical phase in software development that comes after a project has been found feasible. The success of a project heavily depends on effective planning, as unrealistic time and resource estimates can lead to delays, customer dissatisfaction, and even project failure. Therefore, project managers must approach planning with great care, leveraging their knowledge of estimation techniques and past experiences.\n\nHere are the key activities involved in project planning:\n\n1. **Estimation:**\n   - **Cost:** Determining the cost to develop the software product.\n   - **Duration:** Estimating the time required to complete the project.\n   - **Effort:** Calculating the amount of effort needed for development.\n   - Accurate estimates are crucial as they form the basis for subsequent planning activities like scheduling and staffing.\n\n2. **Scheduling:**\n   - After estimating the project parameters, schedules for manpower and resources are developed.\n\n3. **Staffing:**\n   - Planning how the project team will be organized and determining the staffing requirements.\n\n4. **Risk Management:**\n   - Identifying, analyzing, and planning for potential risks.\n\n5. **Miscellaneous Plans:**\n   - Developing plans for quality assurance, configuration management, and other project-related aspects.\n\n### Project Planning Order\nThe planning activities follow a specific order, as shown in Figure 3.1. It begins with size estimation, which informs the effort and duration estimates. These estimates then guide the cost estimation, staffing, scheduling, and other planning activities.\n\n### Sliding Window Planning\nIn large projects, making accurate plans at the beginning is challenging due to uncertainties and changes that occur over time. Sliding window planning addresses this issue by allowing project managers to revise their plans at regular intervals. This staggered planning approach helps avoid making major commitments too early and allows adjustments as more information becomes available.\n\n### SPMP Document (IESRSRPM)\nOnce project planning is complete, all plans are documented in the Software Project Management Plan (SPMP). The SPMP document includes the following sections:\n\n1. **Introduction:**\n   - Objectives, major functions, performance issues, and management constraints.\n\n2. **Project Estimates:**\n   - Historical data, estimation techniques, and estimates for effort, resources, cost, and duration.\n\n3. **Schedule:**\n   - Work breakdown structure, task network representation, Gantt chart, and PERT chart.\n\n4. **Project Resources:**\n   - Details about people, hardware, software, and any special resources required.\n\n5. **Staff Organization:**\n   - Team structure and management reporting mechanisms.\n\n6. **Risk Management Plan:**\n   - Risk analysis, identification, estimation, and abatement procedures.\n\n7. **Project Tracking and Control Plan:**\n   - Metrics to track, tracking plan, and control mechanisms.\n\n8. **Miscellaneous Plans:**\n   - Process tailoring, quality assurance, configuration management, validation and verification, and delivery plans.\n\nIn summary, project planning is a meticulous process that lays the foundation for successful project execution, with careful attention to estimation, scheduling, staffing, risk management, and documentation.",
      "styleAttributes": {},
      "x": 360,
      "y": -300,
      "width": 800,
      "height": 540,
      "color": "2"
    },
    {
      "id": "541490661f9d1573",
      "type": "file",
      "file": "Source Materials/Pasted image 20240907003113.png",
      "styleAttributes": {},
      "x": 4640,
      "y": 3660,
      "width": 840,
      "height": 975
    },
    {
      "id": "45bd4786f2c2ca2c",
      "type": "text",
      "text": "### 3.7.3 Complete COCOMO\n\nThe basic and intermediate COCOMO models have a significant limitation: they treat a software product as a single, homogeneous entity. However, in reality, most large systems are composed of multiple sub-systems, each with unique characteristics and varying levels of complexity. For example, some sub-systems may be considered organic, some semi-detached, and others embedded. These differences can extend beyond just development complexity, including varying reliability requirements and differences in team experience.\n\nThe complete COCOMO model addresses this limitation by allowing the cost of each sub-system to be estimated separately. By breaking down the system into smaller components and estimating the cost for each, the complete COCOMO model reduces the margin of error in the final project cost estimate. Once the individual costs for all sub-systems are calculated, they are summed to determine the total system cost.\n\nFor instance, consider a project involving the development of a distributed management information system (MIS) for an organization with multiple offices across the country. The system could be divided into sub-components like:\n\n- **Database part**: This could be considered semi-detached software.\n- **Graphical User Interface (GUI) part**: This might be categorized as organic software.\n- **Communication part**: This could be treated as embedded software.\n\nEach of these components would have its development costs estimated separately based on its unique characteristics. The total cost of the system would then be the sum of these individual estimates.\n\nAdditionally, the accuracy of the complete COCOMO model can be further enhanced by fine-tuning and validating the parameter values against the organization's historical project data. This helps in obtaining more precise estimates.\n\nWhile estimation models like COCOMO are not entirely accurate and lack full scientific justification, they are crucial for an engineering approach to software project management. Companies generally consider cost estimates to be satisfactory if they fall within about 80% of the final cost. Despite being rough approximations, these models provide a more objective basis for estimation than relying solely on subjective judgments.",
      "styleAttributes": {},
      "x": 2920,
      "y": 3040,
      "width": 1060,
      "height": 520,
      "color": "4"
    },
    {
      "id": "cba85d2960316652",
      "type": "file",
      "file": "Source Materials/Pasted image 20240907002554.png",
      "styleAttributes": {},
      "x": 4620,
      "y": 2720,
      "width": 959,
      "height": 842
    },
    {
      "id": "0609f5a4a29e1e40",
      "type": "text",
      "text": "### 3.7.2 Intermediate COCOMO\n\nThe basic COCOMO model assumes that the effort and development time required for a software project are solely dependent on the product size. However, in reality, various other factors influence both effort and project duration. For example, the sophistication of the development environment and the experience level of the development team can significantly impact the effort required. Recognizing this, the intermediate COCOMO model refines the initial estimates by taking into account additional project parameters.\n\nThe intermediate COCOMO model incorporates 15 cost drivers (multipliers) that are based on different attributes of software development. These cost drivers adjust the initial effort and cost estimates derived from the basic COCOMO model by scaling them up or down, depending on the specific circumstances of the project. For example, if modern programming practices are employed, the estimates are scaled down by applying a cost driver with a value less than 1. Conversely, if the software product has stringent reliability requirements, the estimates are scaled up.\n\nBoehm suggested that project managers rate 15 different parameters on a scale of one to three for a given project. Each rating corresponds to an appropriate cost driver that adjusts the initial estimates accordingly.\n\nThe cost drivers identified by Boehm can be classified into four categories:\n\n1. **Product Attributes**: These include characteristics such as the inherent complexity of the product and the reliability requirements. For instance, a more complex product or one with higher reliability demands would require more effort.\n\n2. **Computer Attributes**: These consider factors like the required execution speed and storage space. A project that demands high computational power or storage capacity may necessitate additional effort.\n\n3. **Personnel Attributes**: These involve the experience and capability of the development team. For example, a team with more experienced developers may complete the project with less effort.\n\n4. **Development Environment Attributes**: These capture the facilities available for development, such as the sophistication of the automation tools (e.g., CASE tools) used in the process. A more advanced development environment can reduce the required effort.\n\nIn essence, the intermediate COCOMO model offers a more nuanced approach to estimation by considering various factors beyond just product size. While the basic ideas of this model have been discussed here, a more detailed exploration of the intermediate COCOMO model can be found in Boehm's original work from 1981.",
      "styleAttributes": {},
      "x": 2660,
      "y": 2420,
      "width": 992,
      "height": 526,
      "color": "4"
    },
    {
      "id": "3de8a68ea2ac47be",
      "type": "file",
      "file": "Source Materials/Pasted image 20240907002041.png",
      "styleAttributes": {},
      "x": 4620,
      "y": 1780,
      "width": 960,
      "height": 783
    },
    {
      "id": "518292cf999ad733",
      "type": "text",
      "text": "\n### 3 .7.1 Basic COCOMO Model\nBoehm postulated that any software development project can be classified into one of\nthe following three categories based on the development complexity—organic, semidetached,\nand embedded. Based on the category of a software development project, he gave different\nsets of formulas to estimate the effort and duration from the size estimate.",
      "styleAttributes": {},
      "x": 2699,
      "y": 1548,
      "width": 513,
      "height": 269,
      "color": "4"
    },
    {
      "id": "f1a00c720cb8f6c4",
      "type": "file",
      "file": "Source Materials/Pasted image 20240907002006.png",
      "styleAttributes": {},
      "x": 4620,
      "y": 1118,
      "width": 960,
      "height": 602
    },
    {
      "id": "1e6ec9998dcaceb9",
      "type": "text",
      "text": "**Three Basic Classes of Software Development Projects**\n\nTo classify a software development project into one of the three categories identified by Boehm (organic, semidetached, and embedded), it's important to consider both the characteristics of the software product and the development team. Here’s a breakdown of these categories:\n\n1. **Organic Projects:**\n   - **Characteristics of the Project:** Deals with developing well-understood application programs.\n   - **Development Team:** The team is typically small and experienced in similar types of projects.\n   - **Example:** A straightforward business application like payroll software, which processes data using simple algorithms.\n\n2. **Semidetached Projects:**\n   - **Characteristics of the Project:** The project may involve moderately complex systems, where the team has a mix of experience levels.\n   - **Development Team:** Includes a mixture of experienced and inexperienced staff. The team members may be familiar with some aspects of the system but might not have extensive experience with all parts of the project.\n   - **Example:** A utility program like a new type of data processing tool where the team has some familiarity but is dealing with new or complex features.\n\n3. **Embedded Projects:**\n   - **Characteristics of the Project:** The software is closely tied to hardware or subject to strict operational constraints.\n   - **Development Team:** Team members may have limited experience with the specific system or domain but are working under stringent requirements.\n   - **Example:** Operating systems or real-time systems, where the software must interact directly with hardware and meet specific performance constraints.\n\n**Relative Complexity:**\n- According to Brooks (1975), utility programs are roughly three times as difficult to write as application programs, and system programs are roughly three times as difficult as utility programs. Thus, the complexity ratio among the three types is 1:3:9.\n\n**Boehm’s Definitions:**\n- **Organic:** Well-understood applications with experienced developers and a small team.\n- **Semidetached:** Projects with a mix of experience levels and some unfamiliar aspects.\n- **Embedded:** Software tightly coupled with hardware or constrained by stringent regulations.\n\n**Person-Month Definition:**\nA **person-month (PM)** is a unit of measurement for effort in software development. It represents the amount of work one person can complete in one month. Here’s what it means:\n\n- **Effort Measurement:** If a project is estimated to require 100 person-months, it does not imply 100 people working for one month or one person working for 100 months. Instead, it reflects the total work effort required.\n- **Work Distribution:** The actual distribution of work might involve varying team sizes over time. A single person might work longer or a team might work shorter, but the total work is still quantified in person-months.\n- **Productivity Losses:** The person-month estimate implicitly accounts for typical productivity losses such as holidays, breaks, and inefficiencies.\n\n**In Practice:**\n- Effort estimation involves predicting how many person-months are required based on project size and complexity. This estimate helps in budgeting and scheduling the project.\n- Understanding the concept of person-months helps in planning how to allocate resources and manage project timelines effectively.\n\nBy classifying a project into one of these categories and understanding the person-month concept, you can better estimate the effort, duration, and cost of software development projects.",
      "styleAttributes": {},
      "x": 3371,
      "y": 1213,
      "width": 1069,
      "height": 443,
      "color": "5"
    },
    {
      "id": "1400229f683bd3aa",
      "type": "file",
      "file": "Source Materials/Pasted image 20240906211713.png",
      "styleAttributes": {},
      "x": 2620,
      "y": 100,
      "width": 900,
      "height": 910
    },
    {
      "id": "ab733b31776a0ac2",
      "type": "file",
      "file": "Source Materials/Pasted image 20240906204525.png",
      "styleAttributes": {},
      "x": 1220,
      "y": -400,
      "width": 1020,
      "height": 300
    },
    {
      "id": "107c4416ffc34e27",
      "type": "text",
      "text": "### 3.13 RISK MANAGEMENT\n\nEvery project faces numerous risks, and effective risk management is crucial to prevent meticulously planned projects from failing. Understanding the difference between potential risks and those that have already manifested is essential. When a risk becomes a reality, it can significantly impact the project's success and timeline, necessitating preemptive identification and management.\n\nRisk management consists of three key activities: risk identification, risk assessment, and risk mitigation.\n\n#### 3.13.1 Risk Management Approaches\n\n**Reactive Approaches**  \nReactive approaches involve taking action only after an adverse event occurs. The focus is on containing the damage and preventing recurrence. For example, if a server crashes, the team may recover data and start regular backups. This approach minimizes damage but may not prevent future issues.\n\n**Proactive Approaches**  \nProactive approaches aim to anticipate risks and take measures to avoid them. If avoidance isn’t possible, contingency plans are established to mitigate impacts. For instance, to address potential manpower turnover, thorough documentation and cross-training might be implemented. Proactive strategies typically incur lower costs and time overruns compared to reactive ones.\n\n#### 3.13.2 Risk Identification\n\nEarly risk identification is critical for effective management. Project managers should anticipate risks akin to listing potential nightmares. Common project risks include vendor reliability, quality of work, and key personnel turnover. \n\nRisks can be categorized into:\n\n- **Project Risks**: Relating to budget, schedule, personnel, resources, and customer-related issues (e.g., schedule slippage).\n- **Technical Risks**: Involving design, implementation, testing, and maintenance problems (e.g., ambiguous specifications).\n- **Business Risks**: Such as developing a product that doesn’t meet market needs.\n\n**Example**: In a satellite-based mobile communication project, identified risks could include cost overruns (project risk), bulky phone designs (business risk), harmful radiation levels (business risk), and difficult call hand-offs (technical risk).\n\nUtilizing a \"disaster list\" from previous projects can aid in identifying potential risks.\n\n#### 3.13.3 Risk Assessment\n\nRisk assessment involves ranking risks based on their potential impact. Each risk is evaluated by:\n\n- **Likelihood of occurrence (r)**\n- **Consequence of the risk (s)**\n\nThe priority for addressing each risk (p) is calculated as:\n\\[ p = r \\times s \\]\nThis prioritization enables the team to focus on the most threatening risks first.\n\n#### 3.13.4 Risk Mitigation\n\nOnce risks are assessed, plans are made to contain the most significant ones. Strategies for risk containment include:\n\n- **Avoidance**: Modifying project constraints to eliminate risks (e.g., adjusting requirements to reduce scope).\n- **Transfer**: Outsourcing components or obtaining insurance.\n- **Reduction**: Planning to limit damage, such as hiring additional personnel if key team members are at risk of leaving.\n\nChoosing the right strategy involves considering the cost of mitigation versus the risk exposure.\n\n**Example**: For schedule slippage, increasing visibility through regular documentation and setting milestones can help manage risks.\n\n#### 3.13.5 Boehm’s Top 10 Risks and Countermeasures\n\nBoehm identifies common project risks and suggests countermeasures:\n\n1. **Personnel Shortfall**: Mitigated by staffing top talent and cross-training.\n2. **Unrealistic Schedules and Budgets**: Addressed through detailed milestones and incremental development.\n3. **Developing the Wrong Functions**: Managed by user surveys and prototyping.\n4. **Incorrect User Interface**: Improved through prototyping and user participation.\n5. **Gold-Plating**: Controlled by requirements scrubbing and cost-benefit analysis.\n6. **Ongoing Requirement Changes**: Handled with incremental development and information hiding.\n7. **Externally-Furnished Component Shortfalls**: Mitigated by benchmarking and inspections.\n8. **Externally Performed Task Shortfalls**: Managed through reference checking and competitive design.\n9. **Real-Time Performance Shortfalls**: Addressed with simulation and benchmarking.\n10. **Straining Computer Science Capabilities**: Mitigated through technical analysis and prototyping.\n\nEffective risk management requires creativity and flexibility from the project manager, adapting strategies as new risks are identified or existing risks evolve.",
      "styleAttributes": {},
      "x": 300,
      "y": 10560,
      "width": 1300,
      "height": 1355
    },
    {
      "id": "8ace71ed5e225148",
      "type": "file",
      "file": "source-images/Pasted image 20240915030227.png",
      "styleAttributes": {},
      "x": -1040,
      "y": 6600,
      "width": 1520,
      "height": 1025
    },
    {
      "id": "6dee460e519d6cb8",
      "type": "text",
      "text": "### Understanding Person-Month and Software Project Estimation\n\n#### **What is a Person-Month?**\n\n**Person-Month (PM)** is a unit used to measure the amount of work required for a project. It's essentially the effort one person can put into a project in a month. Here’s a breakdown of key points about person-months:\n\n1. **Effort Measurement:** An estimate of 100 PM does not mean 100 people working for 1 month or 1 person working for 100 months. Instead, it represents the total effort required to complete the project. The project might involve varying numbers of people over different phases.\n   \n2. **Productivity Losses:** The estimate implicitly accounts for productivity losses such as holidays, breaks, and other non-working times.\n\n3. **Work Allocation:** The distribution of work may vary, and the actual number of people working at any given time can fluctuate, resulting in sharp changes in the plot of effort versus time.\n\n4. **Effort Curve:** The effort curve shows how different numbers of personnel may work at different times during the project. This is depicted in Figure 3.3 (not shown here), which illustrates that personnel may increase or decrease by integral numbers, leading to a jagged curve.\n\n#### **COCOMO Model for Estimating Software Development**\n\nThe **COCOMO (Constructive Cost Model)** is used to estimate the effort and time required for software development. The basic COCOMO model includes:\n\n1. **Effort Estimation:**\n   \\[\n   \\text{Effort} = a_1 \\times (\\text{KLOC})^{a_2} \\text{ PM}\n   \\]\n   - **KLOC**: Size of the software in Kilo Lines of Code.\n   - **a1, a2**: Constants for different types of software (organic, semidetached, embedded).\n\n2. **Development Time Estimation:**\n   \\[\n   \\text{Tdev} = b_1 \\times (\\text{Effort})^{b_2} \\text{ months}\n   \\]\n   - **Tdev**: Time to develop the software.\n   - **b1, b2**: Constants for different software types.\n\n#### **Categories and Formulas**\n\n1. **Organic Software:**\n   - **Effort:** \\( \\text{Effort} = 2.4 \\times (\\text{KLOC})^{1.05} \\text{ PM} \\)\n   - **Development Time:** \\( \\text{Tdev} = 2.5 \\times (\\text{Effort})^{0.38} \\text{ months} \\)\n\n2. **Semi-Detached Software:**\n   - **Effort:** \\( \\text{Effort} = 3.0 \\times (\\text{KLOC})^{1.12} \\text{ PM} \\)\n   - **Development Time:** \\( \\text{Tdev} = 2.5 \\times (\\text{Effort})^{0.35} \\text{ months} \\)\n\n3. **Embedded Software:**\n   - **Effort:** \\( \\text{Effort} = 3.6 \\times (\\text{KLOC})^{1.20} \\text{ PM} \\)\n   - **Development Time:** \\( \\text{Tdev} = 2.5 \\times (\\text{Effort})^{0.32} \\text{ months} \\)\n\n#### **Example Problem and Solution**\n\n**Problem 3.2:**\n- **Given:**\n  - Size of organic software: 32,000 lines of code (32 KLOC)\n  - Average salary: `15,000 per month\n\n- **Calculations:**\n\n1. **Effort:**\n   \\[\n   \\text{Effort} = 2.4 \\times (32)^{1.05} = 91 \\text{ PM}\n   \\]\n\n2. **Nominal Development Time:**\n   \\[\n   \\text{Tdev} = 2.5 \\times (91)^{0.38} = 14 \\text{ months}\n   \\]\n\n3. **Cost:**\n   \\[\n   \\text{Staff Cost} = 91 \\text{ PM} \\times \\text{`15,000} = \\text{`1,465,000}\n   \\]\n\n#### **Implications of Effort and Duration Estimates**\n\n- **Cost and Duration Relationship:** If a project is completed in less time than estimated, costs increase due to inefficiencies and idle time. Conversely, extending the project duration typically does not reduce costs significantly.\n\n- **Optimal Team Size:** The ideal team size allows all members to be continuously productive without idle time. Deviating from this size impacts the project cost and efficiency.\n\n- **Staffing Issues:** Simple division of effort by duration to determine staff size can lead to delays and cost overruns. Proper staffing requires careful consideration of the project’s requirements and development phases.\n\nUnderstanding these concepts helps in effective project management and accurate estimation of development efforts and costs.",
      "styleAttributes": {},
      "x": 3371,
      "y": 1683,
      "width": 1069,
      "height": 480,
      "color": "5"
    },
    {
      "id": "3d5d3e17cfdb4c3f",
      "type": "text",
      "text": "# Organisation Structure\n\n#### 1. **Functional Format**:\nIn a **functional format**, staff members are divided based on their specific roles or functions, such as analysis, design, coding, or testing. Each functional team works on their respective phase of the project and passes the product to the next team.\n\n- **Advantages**:\n  - **Efficient Project Staffing**: Personnel are brought into a project when needed and returned to their functional group afterward, reducing idle time and improving resource utilization.\n  - **High-Quality Documentation**: Since the work is passed between functional teams, high-quality documentation is mandatory to ensure smooth transitions between phases.\n  - **Job Specialization**: Team members develop expertise in specific areas, becoming specialists in their roles.\n  - **Manpower Turnover**: With good documentation and specialized roles, the impact of developers leaving a project mid-way is minimized, as replacements from the functional group can quickly adapt.\n\n- **Disadvantages**:\n  - **Lack of Job Rotation**: Developers may become too specialized and miss out on experiencing different aspects of the software life cycle.\n  - **Difficulty in Staffing for Small Projects**: The functional format is more suited for organizations handling many projects. Small organizations might struggle to implement this structure effectively.\n\n#### 2. **Project Format**:\nIn the **project format**, teams are assigned to a project from the beginning and remain with it throughout the entire life cycle. Each team handles all the phases of the project—requirements, design, coding, and testing.\n\n- **Advantages**:\n  - **Job Rotation**: Team members gain experience in different roles as they handle all life cycle phases of the project.\n  - **Continuity**: Since the same team handles the entire project, communication is easier, and transitions between phases are smoother.\n\n- **Disadvantages**:\n  - **Inefficient Resource Utilization**: Project teams are typically formed at the beginning of a project, leading to idle time during the early phases and high pressure during later phases.\n  - **No Specialization**: Developers do not get the opportunity to specialize in specific areas, as they are involved in all phases.\n\n#### 3. **Matrix Format**:\nThe **matrix format** combines aspects of both the functional and project structures. Functional specialists are assigned to different projects as needed, reporting to both a project manager and a functional manager.\n\n- **Strong Matrix**: Functional managers have the authority to assign workers to projects, and project managers must accept the assigned staff.\n- **Weak Matrix**: Project managers control the project budget and can reject workers from functional groups or hire outside workers.\n\n- **Advantages**:\n  - Combines the strengths of functional and project formats, allowing for efficient staffing while providing job rotation.\n\n- **Disadvantages**:\n  - **Authority Conflicts**: Conflicts can arise between functional and project managers over staff allocation.\n  - **Frequent Shifting of Workers**: In strong matrix organizations, functional managers may frequently reassign staff to address crises, disrupting project work.\n\n---\n\nOverall, the choice of organization structure depends on the size of the organization, the number of projects being handled, and the need for specialization or flexibility in project execution. The matrix format attempts to balance the benefits of both functional and project-based approaches, though it introduces complexity in management.",
      "styleAttributes": {},
      "x": 400,
      "y": 7800,
      "width": 1365,
      "height": 583,
      "color": "6"
    },
    {
      "id": "c3db2f566b93f302",
      "type": "file",
      "file": "source-images/Pasted image 20240915031646.png",
      "styleAttributes": {},
      "x": 1840,
      "y": 7800,
      "width": 958,
      "height": 560
    },
    {
      "id": "89bf64a6d3578623",
      "type": "file",
      "file": "source-images/Pasted image 20240915031700.png",
      "styleAttributes": {},
      "x": 1840,
      "y": 8383,
      "width": 1080,
      "height": 477
    },
    {
      "id": "0996c259c4fa78a4",
      "type": "text",
      "text": "# Organisation and Team Structures\n\nIn software development organizations, managing multiple projects simultaneously requires careful structuring of both the organization and the project teams. Two key considerations are:\n\n1. **Organisation Structure**: How the entire organization is structured.\n2. **Team Structure**: How the individual project teams are structured.\n\nThere are three primary ways to structure a software development organization: **functional**, **project**, and **matrix** formats. Each format has its own advantages and disadvantages, depending on the nature of the projects and the organization.",
      "styleAttributes": {},
      "x": -920,
      "y": 8560,
      "width": 740,
      "height": 360,
      "color": "1"
    },
    {
      "id": "6c54f03aa2c51d89",
      "type": "text",
      "text": "### 3.11.2 Team Structure\n\nTeam structure refers to how individual project teams are organized within a software development organization. There are several ways in which project teams can be structured, depending on the size and complexity of the project. Three common formal team structures are discussed below: **Chief Programmer**, **Democratic**, and **Mixed Control** team organizations. Each structure has its own strengths and weaknesses, and certain projects may benefit more from one type of structure than another.\n\n#### 1. Chief Programmer Team\n\nIn the **Chief Programmer** structure, a senior engineer (the \"chief programmer\") provides technical leadership and has authority over the team. The chief programmer breaks down the project into smaller tasks, assigns them to team members, and integrates the work. The hierarchy in this setup is strictly defined, with the chief programmer overseeing and verifying the output of the entire team.\n\n**Advantages:**\n- **Efficiency:** For simple, well-understood problems, the chief programmer can quickly design a solution, and the team can focus on coding and implementing it.\n- **Clear Leadership:** Having a single leader reduces confusion and makes decision-making more streamlined.\n\n**Disadvantages:**\n- **Morale Issues:** Team members may feel less empowered and experience lower morale due to constant supervision.\n- **Single Point of Failure:** The project may suffer if the chief programmer leaves or is unavailable, as much of the responsibility is centralized.\n- **Limited Creativity:** Team members may feel restricted in contributing original ideas due to the top-down approach.\n\nThis structure is most suitable for smaller, well-understood projects where early completion is crucial and the complexity is within the grasp of a single individual.\n\n#### 2. Democratic Team\n\nThe **Democratic Team** structure has no formal hierarchy within the team. While an administrative manager may be present to handle coordination, the technical leadership often rotates among different members of the team depending on the task at hand. This setup fosters collaboration and encourages all members to contribute equally.\n\n**Advantages:**\n- **High Morale and Job Satisfaction:** Since all members contribute to the decision-making process, they have higher job satisfaction and are less likely to leave the project.\n- **Egoless Programming:** Team members share their work and review each other’s contributions, which leads to better overall quality of output.\n- **Better for Complex Problems:** For projects that are less understood, democratic teams can come up with better solutions due to collective thinking and brainstorming.\n\n**Disadvantages:**\n- **Lower Productivity:** Decision-making can be slower due to discussions, and time may be wasted on trivial arguments.\n- **Unsuitable for Large Projects:** Pure democratic teams can become chaotic and inefficient when the team size grows beyond five or six developers.\n\nThis structure works best for research-oriented projects and situations where innovation is needed to solve poorly defined problems.\n\n#### 3. Mixed Control Team Organization\n\nThe **Mixed Control** structure combines elements of both democratic and chief programmer team setups. It incorporates a hierarchical reporting structure with some democratic features, allowing for both technical leadership and collaborative decision-making. \n\nIn this setup, senior developers form a democratic group to break down the project into smaller tasks, which are then handled by sub-teams. Each sub-team, organized in a more democratic fashion, works on a specific portion of the project.\n\n**Advantages:**\n- **Combines Strengths:** The structure balances the clear leadership of the chief programmer model with the collaborative problem-solving of the democratic model.\n- **Scalability:** The mixed control model is particularly suited to larger and more complex projects, where sub-teams can work independently on different parts of the project.\n- **Popular in Industry:** Due to its flexibility, the mixed control structure is widely used in software companies, allowing for better handling of complex projects while fostering some level of autonomy among developers.\n\n**Disadvantages:**\n- **Potential for Conflicts:** Conflicts may arise between the hierarchical and democratic elements, especially regarding decision-making and task allocation.\n  \nThis structure is ideal for large projects with multiple interdependent components that require both careful planning and creative problem-solving.\n\n---\n\nEach team structure has its ideal use cases depending on project complexity, team size, and organizational culture. By selecting the appropriate structure, software organizations can optimize productivity, innovation, and team satisfaction.",
      "styleAttributes": {},
      "x": 400,
      "y": 8980,
      "width": 1340,
      "height": 1300,
      "color": "6"
    },
    {
      "id": "cd488b51aac84118",
      "type": "file",
      "file": "source-images/Pasted image 20240915031557.png",
      "styleAttributes": {},
      "x": 1840,
      "y": 8980,
      "width": 840,
      "height": 658
    },
    {
      "id": "633cc2e05650b2f6",
      "type": "file",
      "file": "source-images/Pasted image 20240915031626.png",
      "styleAttributes": {},
      "x": 2720,
      "y": 8980,
      "width": 1180,
      "height": 620
    },
    {
      "id": "7ff07b93a2225899",
      "type": "file",
      "file": "source-images/Pasted image 20240915031610.png",
      "styleAttributes": {},
      "x": 1840,
      "y": 9760,
      "width": 1010,
      "height": 600
    }
  ],
  "edges": [
    {
      "id": "a245665e600a939a",
      "styleAttributes": {},
      "fromNode": "ab425b93022cdc22",
      "fromSide": "right",
      "toNode": "2597214111e33079",
      "toSide": "left",
      "color": "1"
    },
    {
      "id": "8d4a7ff4584105f3",
      "styleAttributes": {},
      "fromNode": "2597214111e33079",
      "fromSide": "right",
      "toNode": "ab733b31776a0ac2",
      "toSide": "left",
      "color": "2"
    },
    {
      "id": "36eb955be1b7d4aa",
      "styleAttributes": {},
      "fromNode": "3b122a63a709e73a",
      "fromSide": "right",
      "toNode": "41ba5c3df05a2681",
      "toSide": "left",
      "color": "2"
    },
    {
      "id": "3696782fe5c18461",
      "styleAttributes": {},
      "fromNode": "3b122a63a709e73a",
      "fromSide": "right",
      "toNode": "e4f4e23185bb37af",
      "toSide": "left",
      "color": "2"
    },
    {
      "id": "b30fa39b1d7395de",
      "styleAttributes": {},
      "fromNode": "e4f4e23185bb37af",
      "fromSide": "right",
      "toNode": "1400229f683bd3aa",
      "toSide": "left",
      "color": "3"
    },
    {
      "id": "d913ab34b9452d87",
      "styleAttributes": {},
      "fromNode": "18e9b6da4880d680",
      "fromSide": "right",
      "toNode": "518292cf999ad733",
      "toSide": "left",
      "color": "3"
    },
    {
      "id": "5672c21641dfac74",
      "styleAttributes": {},
      "fromNode": "518292cf999ad733",
      "fromSide": "right",
      "toNode": "1e6ec9998dcaceb9",
      "toSide": "left",
      "color": "4"
    },
    {
      "id": "752fb414d262208a",
      "styleAttributes": {},
      "fromNode": "518292cf999ad733",
      "fromSide": "right",
      "toNode": "6dee460e519d6cb8",
      "toSide": "left",
      "color": "4"
    },
    {
      "id": "eacc3a968d9261b5",
      "styleAttributes": {},
      "fromNode": "6dee460e519d6cb8",
      "fromSide": "right",
      "toNode": "3de8a68ea2ac47be",
      "toSide": "left",
      "color": "5"
    },
    {
      "id": "ca55987904f074a8",
      "styleAttributes": {},
      "fromNode": "6dee460e519d6cb8",
      "fromSide": "right",
      "toNode": "f1a00c720cb8f6c4",
      "toSide": "left",
      "color": "5"
    },
    {
      "id": "021d772189a223bf",
      "styleAttributes": {},
      "fromNode": "6dee460e519d6cb8",
      "fromSide": "right",
      "toNode": "cba85d2960316652",
      "toSide": "left",
      "color": "5"
    },
    {
      "id": "dfdfb20823200505",
      "styleAttributes": {},
      "fromNode": "6dee460e519d6cb8",
      "fromSide": "right",
      "toNode": "541490661f9d1573",
      "toSide": "left",
      "color": "5"
    },
    {
      "id": "eef31f3c1adcae0d",
      "styleAttributes": {},
      "fromNode": "18e9b6da4880d680",
      "fromSide": "right",
      "toNode": "0609f5a4a29e1e40",
      "toSide": "left",
      "color": "3"
    },
    {
      "id": "6d00683b95b80661",
      "styleAttributes": {},
      "fromNode": "18e9b6da4880d680",
      "fromSide": "right",
      "toNode": "45bd4786f2c2ca2c",
      "toSide": "left",
      "color": "3"
    },
    {
      "id": "389bed530338202a",
      "styleAttributes": {},
      "fromNode": "387ac2aa506514d8",
      "fromSide": "right",
      "toNode": "7993648dcc6f3f73",
      "toSide": "left"
    },
    {
      "id": "048acc7dea39f47d",
      "styleAttributes": {},
      "fromNode": "8938a57a81723e81",
      "fromSide": "right",
      "toNode": "9bfc965cc1c7d7c4",
      "toSide": "left"
    },
    {
      "id": "6791066dfea8f1f9",
      "styleAttributes": {},
      "fromNode": "387ac2aa506514d8",
      "fromSide": "right",
      "toNode": "8938a57a81723e81",
      "toSide": "left",
      "color": "1"
    },
    {
      "id": "767363ae65f341e4",
      "styleAttributes": {},
      "fromNode": "0996c259c4fa78a4",
      "fromSide": "right",
      "toNode": "3d5d3e17cfdb4c3f",
      "toSide": "left",
      "color": "1"
    },
    {
      "id": "6c68a6c2c0c22f47",
      "styleAttributes": {},
      "fromNode": "0996c259c4fa78a4",
      "fromSide": "right",
      "toNode": "6c54f03aa2c51d89",
      "toSide": "left",
      "color": "1"
    },
    {
      "id": "f264ef1009e6658b",
      "styleAttributes": {},
      "fromNode": "b31b2709ba5575fa",
      "fromSide": "right",
      "toNode": "18e9b6da4880d680",
      "toSide": "left",
      "color": "2"
    },
    {
      "id": "7b323d65027ea4e2",
      "styleAttributes": {},
      "fromNode": "b31b2709ba5575fa",
      "fromSide": "right",
      "toNode": "5532fa8dde242548",
      "toSide": "left",
      "color": "2"
    }
  ],
  "metadata": {}
}